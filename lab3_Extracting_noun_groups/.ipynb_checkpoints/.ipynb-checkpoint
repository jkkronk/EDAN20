{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 3 - Extracting noun groups using machine learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The objectives of this assignment are to:\n",
    "\n",
    "- Write a program to detect partial syntactic structures\n",
    "- Understand the principles of supervised machine learning techniques applied to language processing\n",
    "- Use a popular machine learning toolkit: scikit-learn\n",
    "- Write a short report of 1 to 2 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a training and a test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "b_train_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt\").read() # Open file and read\n",
    "train_text = str(b_train_text,'utf-8')\n",
    "b_test_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt\").read() # Open file and read\n",
    "test_text = str(b_test_text,'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEXT EXAMPLE TRAIN---\n",
      " Confidence NN B-NP\n",
      "in IN B-PP\n",
      "the DT B-NP\n",
      "pound NN I-NP\n",
      "is VBZ B-VP\n",
      "widely RB I-VP\n",
      "expected VBN I-VP\n",
      "to TO I-VP\n",
      "take VB I-VP\n",
      "another DT B-NP\n",
      "sharp JJ I-NP\n",
      "dive NN I-NP\n",
      "if IN B-SBAR\n",
      "trade NN B-NP\n",
      "figur \n",
      " ---TEXT EXAMPLE TEST--- \n",
      " Rockwell NNP B-NP\n",
      "International NNP I-NP\n",
      "Corp. NNP I-NP\n",
      "'s POS B-NP\n",
      "Tulsa NNP I-NP\n",
      "unit NN I-NP\n",
      "said VBD B-VP\n",
      "it PRP B-NP\n",
      "signed VBD B-VP\n",
      "a DT B-NP\n",
      "tentative JJ I-NP\n",
      "agreement NN I-NP\n",
      "extending VBG B-\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEXT EXAMPLE TRAIN---\\n\",train_text[:200], \"\\n ---TEXT EXAMPLE TEST--- \\n\",test_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical algorithms for language processing start with a so-called baseline. The baseline figure corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the baseline proposed by the organizers of the CoNLL 2000 shared task (In the Results Sect.). What do you think of it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They get pretty high score overall but no method applying advanced ml methods with deep neural networks. (Which is understandable since the conference was held at year 2000.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement this baseline program. You may either create a completely new program or start from an existing program that you will modify. https://github.com/pnugues/ilppp/tree/master/programs/labs/chunking/chunker_python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train function so that it computes the chunk distribution for each part of speech. You will use the train file to derive your distribution and you will store the results in a dictionary. Below, you have an excerpt of the expected results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.read_sentences(train_text)\n",
    "sentences_train = train_text.split('\\n\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.split_rows(train_corpus, column_names)\n",
    "train_corpus = []\n",
    "for sentence in sentences_train:\n",
    "    rows = sentence.split('\\n')\n",
    "    sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "    train_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.read_sentences(train_text)\n",
    "sentences_test = test_text.split('\\n\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.split_rows(train_corpus, column_names)\n",
    "test_corpus = []\n",
    "for sentence in sentences_test:\n",
    "    rows = sentence.split('\\n')\n",
    "    sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "    test_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       " {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       " {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       " {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       " {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       " {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       " {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       " {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       " {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       " {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       " {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': '.', 'pos': '.', 'chunk': 'O'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(corpus):\n",
    "    \"\"\"\n",
    "    Computes the part-of-speech distribution\n",
    "    in a CoNLL 2000 file\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row == {}:\n",
    "                continue\n",
    "            if row['pos'] in pos_cnt:\n",
    "                pos_cnt[row['pos']] += 1\n",
    "            else:\n",
    "                pos_cnt[row['pos']] = 1\n",
    "    return pos_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(corpus):\n",
    "    \"\"\"\n",
    "    Computes the chunk distribution by pos\n",
    "    The result is stored in a dictionary\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = count_pos(corpus)\n",
    "\n",
    "    # We compute the chunk distribution by POS\n",
    "    \"\"\"\n",
    "    Fill in code to compute the chunk distribution for each part of speech\n",
    "    \"\"\"\n",
    "    chunk_dist = {key: {} for key in pos_cnt.keys()}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row == {}:\n",
    "                continue\n",
    "            if row['chunk'] in chunk_dist[row['pos']]:\n",
    "                chunk_dist[row['pos']][row['chunk']] += 1\n",
    "            else:\n",
    "                chunk_dist[row['pos']][row['chunk']] = 1\n",
    "        \n",
    "    print(\"Example of probdist for JJR: \", chunk_dist['JJR'])\n",
    "    # We determine the best association\n",
    "    \"\"\"\n",
    "    Fill in code so that for each part of speech, you select the most frequent chunk.\n",
    "    You will build a dictionary with key values:\n",
    "    pos_chunk[pos] = most frequent chunk for pos\n",
    "    \"\"\"\n",
    "    pos_ret = {key: \"\" for key in pos_cnt.keys()}\n",
    "    for pos in chunk_dist:\n",
    "        max_value = 0\n",
    "        max_chunk = \"\"\n",
    "        for chunk in chunk_dist[pos]:\n",
    "            if max_value < chunk_dist[pos][chunk]:\n",
    "                max_value = chunk_dist[pos][chunk]\n",
    "                max_chunk = chunk\n",
    "        pos_ret[pos] = max_chunk\n",
    "    \n",
    "    return pos_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of probdist for JJR  {'B-NP': 382, 'B-ADJP': 111, 'I-ADJP': 45, 'B-ADVP': 63, 'I-ADVP': 17, 'B-VP': 2, 'I-NP': 204, 'I-VP': 11, 'O': 16, 'B-PP': 2}\n",
      "Example of train model for NN:  I-NP\n"
     ]
    }
   ],
   "source": [
    "model = train(train_corpus)\n",
    "\n",
    "print(\"Example of train model for NN: \",model['NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, corpus):\n",
    "    \"\"\"\n",
    "    Predicts the chunk from the part of speech\n",
    "    Adds a pchunk column\n",
    "    :param model:\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    We add a predicted chunk column: pchunk\n",
    "    \"\"\"\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if 'pos' in row:\n",
    "                row['pchunk'] = model[row['pos']]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': 'In', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'}, {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'same', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'statement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': ',', 'pos': ',', 'chunk': 'O', 'pchunk': 'O'}, {'form': 'US', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'Facilities', 'pos': 'NNPS', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'also', 'pos': 'RB', 'chunk': 'B-ADVP', 'pchunk': 'B-ADVP'}, {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'}, {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'had', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'}, {'form': 'bought', 'pos': 'VBN', 'chunk': 'I-VP', 'pchunk': 'I-VP'}, {'form': 'back', 'pos': 'RB', 'chunk': 'B-ADVP', 'pchunk': 'B-ADVP'}, {'form': '112,000', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'I-NP'}, {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'}, {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'common', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'shares', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'}, {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'private', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'transaction', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(model, test_corpus)\n",
    "\n",
    "print(predicted[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
