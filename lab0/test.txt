Simulation  of  Fictitious  Disease  usingReinforcement  LearningJonatan KronanderTranslational Neuromodeling Course ProjectETH ZurichAbstract— This project report describes simulations of an agentwith  a  fictitious  disease  and  explain  symptoms,  assuming  asimple  reinforcement  learning  model  for  the  agent’s  learningand  decision  making.  Models  of  reinforcement  learning  canhelp  understanding  of  how  the  brain  deals  with  learning  anddecision making and with this, understanding there might comeimproved diagnostics and treatment for psychological diseases.Results of this project report show how symptoms of an agentwith  a  disease  can  be  explained  by  model  parameters.I.  INTRODUCTIONReinforcement  learning  (RL)  is  an  topic  first  sprung  fromcomputational  psychology,  neuroscience  and  computer  sci-ence.  [1]  In  RL,  it  is  studied  how  an  agent  learns  theoptimal behaviour in an unknown environment to maximizefuture  reward  and  minimize  future  punishment.  Potentialadvantages   are   big   if   a   model   for   the   brain’s   learningand  decision  problems  can  be  explained  by  a  RL  model.By  linking  model  parameters  to  brain  mechanism,  thereare  potential  to  see  which  parameters  and  mechanisms  iscausing certain abnormal symptoms. Better understanding ofwhat is causing different abnormal symptoms gives hope toimproved diagnosis and treatment for diseases linked to thesesymptoms.  [2]  [3]  Studies  have  found  connections  betweenRL:s reward/punishments and dopamine, potentially linkingRL  models  to  disaeses  such  as  schizophrenia,  Parkinson’sdisease, Tourette’s syndrome and drug addiction. [4] [5] [6]As part of recent artificial intelligence and machine learningachievements,  the  RL  community  have  grown  in  recentyears  and  many  new  models  for  learning,  often  using  deepneural nets, have gotten proposed. Instead, in this project, asimple  RL  model,  Q-learning,  will  be  used  as  an  cognitivelearning model for the agent to solve optimal behaviour fortwo  different  environments.  In  order  to  explain  parametersand  link  these  to  symptoms,  its  crucial  to  have  a  goodunderstanding  of  the  implemented  cognitive  model,  hencethe  simple  RL  algorithm.  Motivation  of  this  project  is  to,with  a  Q-learning  model,  simulate  cognitive  diseases  andsymptoms that then can be linked to model parameters. Anadvanced  model  for  the  agents  perception  was  not  studiedin this project. Furthermore, this project report assumes thatthe agent have full perception of the environment at all time,i.e. the environment is always fully observable.This  report  is  an  part  of  a  project  in  Translational  Neu-romodeling  Course  at  ETH  Zurich  where  the  goal  of  theproject was to model a fictitious disease. In the first section,an  explanation  of  environment  and  agents  model  will  bepresented.  Further  in  results  simulations  of  agents  learningbehaviour,  both  an  normal  and  an  abnormal  agent,  will  beshown.  Lastly,  simulations  of  the  fictitious  diseases  will  bediscussed.II.  METHODThis  project  simulated  an  agents  learning  behaviour  in  twodifferent environments. From these simulations, conclusionsof  which  learning  symptoms  was  cause  from  which  modelparameters.  To  draw  conclusions  from  learning  behaviourand  symptoms,  first  optimal  learning  parameters  had  to  befound. A normal agent was considered to have optimal learn-ing  behaviour.  Agents  showing  slow  learning  or  unable  tosolve  problem  were  considered  having  abnormal  symptomsand these agents was regarded as having a fictitious disease.Models of describing reinforcement often uses Markov deci-sion process (MDP) [7], where the agent have a set of actionsand  environment  have  a  set  of  states.  In  each  discrete  timestep, t, the environment is in stateStand the agent executesan action,At. The action will move the environment into thenext state,St+1and the process will give a reward, R, to theagent.A. ENVIRONMENT1) MAZE:The  first  simulated  environment  was  a  mazeproblem.  The  agent  was  expected  to  find  the  optimal  pathfrom point A to point B intdiscrete time steps. If the agentfound point B it got rewarded. For each step the agent took, itgot a punishment and if the agent took a non-permitted step,e.i. a step out of bounds or into a wall, the agent would servea greater punishment. The environment was fully observablefor the agent at all time stept. The agent could localise itselfbut it did not have any information about which actions werepermitted or not.The agent could choose one of four different actions in eachstate, (left, right, up and down). To make the maze problemstochastic and harder for the agent, the maze chose a randomunknown  action  for  the  agent,  with  probabilityp.  Figure  1shows  the  maze  that  were  used  for  simulations  in  result.This  maze  problem  tested  how  fast  the  agent  understoodwhich  was  the  optimal  policy  in  each  state  to  maximize
future  reward.  The  maze  environment  implementation  wasbuilt on Samy Zafranys Maze environment. [8]Fig.  1.Figure  shows  the  maze  that  where  used  for  simulations.  Startingpoint  A  is  located  in  right  upper  corner  and  reward  point  B  is  located  inleft down corner.2) CHAIN:The  second  environment  was  a  "chain",  bestdescribed  by  an  MDP,  as  in  Figures  2  and  3.  In  this  chainenvironment,  the  agent  got  rewards  in  two  different  states.Either a small reward in the starting state or a large rewardin the state farthest away from the starting state. The agentcould take either of two actions: Actiona0which resulted inthe agent going down towards the large reward state or Ac-tiona1which resulted in the agent going back to the startingpoint and granted a small reward. There was also a stochasticcomponent in the environment as it would by probabilitypmake the agent unknowingly "slip" and perform the oppositeaction  than  requested.  This  chain  environment  tested  if  theagent got stuck in the small reward state. Showing how goodthe agent valued future award against immediate award. Thisenvironment  was  implemented  directly  from  Openai  toolkitgym. [9]Fig. 2.    Figure showing the chain that where used for simulations. Startingstate and small reward state isS0and large reward state isS3. The agentcould  either  take  actiona0,  "down"  ora1"back  to  start".  In  stateS0theagent would receive a small reward and in stateS3the agent would receivea big reward. This chain can also be described as in Figure 3.B. AGENTIn  RL,  there  are  two  different  alternative  ways  to  describehow  a  model  tries  to  predict  which  action  is  best  in  eachstate.  The  first  alternative  is  to  use  a  model-based  learningalgorithm  where  the  agent  tries  to  build  a  forward  modelof  the  environment,  e.i.  the  agent  will  map  the  MDP  byFig.  3.In  this  figure  stateS0is  the  top  box  and  stateS3is  the  bottombox. Actiona0correspond to take one step down and actiona1correspondto go back to the top box.predicting the probabilities that an action is leading to a nextspecific  state  and  reward.  Instead,  the  second  alternative  isto  map  what  predicted  reward  the  model  gets  given  a  stateand action pair, or Q-value. It is not clear weather the brainuses a model based or model free algorithm to solve learningtask but for simplification a model free algorithm was usedto describe learning, more exactly the Q-learning algorithm.[10]Q-learning  is  an  simple  reinforcement  learning  algorithmwhich  was  first  introduced  by  Watkins  in  1989.  [11]  Thealgorithm  predicts  the  optimal  action  policy  to  maximizingfuture  reward  for  each  state  in  a  MDP.  From  the  startof  learning,  the  agent  will,  without  priors,  take  randomactions  and  from  each  action,  update  its  policy  from  thereward outcome for each Q-value, (state, action). The updateequation  for  the  agent’s  policyQ(s,a),  seen  in  Equation  1and  2,  is  a  central  part  of  the  Q-learning  algorithm  sinceits  learning  the  agent  how  to  get  maximal  reward  fromthe  sampled  MDP.  The  update  equation  have  two  differentparameters  that  can  be  changed,  learning  rate  and  discountfactor.  The  reward  parameter  is  given  by  the  environment.The  learning  rate  corresponds  to  how  much  the  agent  willupdate  its  policy,  e.i.  how  much  the  agent  believes  theenvironment rewards. If the learning rate is too high it willalways believe the environment reward and the agent can endup in local maximum, missing the global maximum policy. Inthe other hand, a low learning rate will make the agent a slowlearner.  In  a  purely  non  stochastic  environment  the  optimallearning rate would be equal to one. [12] The discount factorcorrespond to how much the agent counts current reward overfuture  reward.  This  is  vital  since,  in  theory,  the  agents  thefuture  rewards  is  less  valuable  than  immediate  rewards  forexploring. [13]Q(st,at) =Q(st,at)+α(ρ+γ(max(Q(st+1,:))−Q(st,at)))(1)α=Learning rate, ρ=Reward, γ=Discount factor   (2)
The  agent  also  have  an  exploration/expropriation  problem,e.i.  should  the  agent  explore  the  MDP  or  should  it  takeits  predicted  optimal  reward.  There  are  several  differentsolutions  to  this  exploration  problem.  [14]  In  this  projectthe  implemented  solution  was  Boltzmann  exploration,  alsoknown as softmax exploration. Boltzmann exploration, Equa-tion  3  and  4,  creates  a  probability  distribution  over  allpossible  actions  with  higher  probability  for  actions  withhigher predicted rewards. The agent’s action is then sampledfrom  the  distributionπ.  With  this  probability  distribution,the agent will explore more when not knowing which actionthat will result in the best reward. If the temperature factorTis  high,  the  probability  distributionπwill  be  more  flatand the agent will act more at random, i.e. exploring more.In  simulations  that  were  made  the  temperature  factor  waslowered  after  each  episode  simulating  that  the  agent  goesfrom  more  exploration  to  more  expropriation  after  eachepisode.π(st,at) =exp(Q(st,a)/T)∑mi=0exp(Q(st,ai)/T)(3)m=Nbr of actions,T=Temperature,π=Policy distribution(4)C. SIMULATIONSFor the maze environment the agent had 500 steps to reachthe  reward  until  it  failed.  The  agent  was  given  reward  100when  reaching  Point  B.  Punishments  was  -0.25  when  theagent took action where it had been before, -0.04 for any stepand -0.4 for a non permitted step. An optimal reward wouldyield  a  maximum  reward  of6.44.  The  chain  environmentgave  the  agent  50  time  steps  in  each  episode.  The  agentwas served a reward of 1 at starting state and 1000 at States3. This resulted in a maximal reward of47000if the agentnever would have slipped. Since both the maze and the chainenvironment is stochastic the stationary reward will have highvariation.III.  RESULTA. MAZE SIMULATIONS1) NORMAL LEARNING:An optimal and normal learningbehaviour was obtained with parameters learning rate= 0.3and  discount  rate= 0.6as  seen  in,  Figure  4.  The  agentwas  able  to  find  a  solution  in  almost  125  episodes.  Thelarge  variance  after  125  episodes  can  be  explained  by  thestochastic factor in the maze.2) LEARNING WITH DISEASE:A  disease  1,  resulted  inlearning  behaviour  seen  in  Figures  5  &  6.  Here  it  can  benoticed  that  the  agent  finds  a  solutions  almost  as  fast  as  anormal agent, if they have same learning rate, but the agentwith the disease does not seem sure of its own solution. Onecan clearly see a pattern where the agent get many rewardsFig. 4.    A normal agent was obtained using parameters learning rate = 0.3and discount factor = 0.6.around -100 and -120. This reward pattern correspond to theagent avoiding taking steps out of bounds following its ownpath back and forth not reaching the goal and large reward,i.e. the agent gets stuck in a local reward maximum, or takinga non permitted action. The disease 1 was caused by a lowdiscount factor.Fig. 5.    Simulation of a agent with disease 1, in maze, where it never seemto  find  the  optimal  solution.  Constantly  doubt  itself  and  regularly  gettingstuck in local maximum.A  disease  2,  where  the  agent  converges  similarly  as  anormal agent but after longer time (episodes) can be seen inFigures 7, 8 & 9. Notice that these agents had very varyingparameters,  both  a  high  discount  factor  and  high  and  lowlearning rate resulted in almost same learning behaviour.An agent with the last simulated disease, disease 3, was notable  to  solve  the  learning  task  in  reasonable  time,  as  seenin  Figure  10.  This  disease  was  caused  by  very  off  modelparameters.B. CHAIN SIMULATIONS1) NORMAL LEARNING:An  optimal  learning  behaviourwas  found  to  not  depend  that  much  on  learning  rate  ratherthe discount factor. A learning rate of 0.9 and discount factorof 0.9 was found to yield a nearly optimal learning behaviouras  in  Figure  11,  resulting  in  an  almost  immediate  optimal
Fig. 6.Another example of disease 1, in maze, but with slower learningbehaviour.Fig. 7.    Example one of an agent, in maze, with disease 2 where the agentis learning slowly.behaviour.  The  high  variance  in  stationary  result  is  causedby the stochastic environment.2) LEARNING WITH DISEASE:Since  there  only  wheretwo  actions  in  this  environment  the  number  of  diseaseswhere few. Either agent found the optimal behaviour (alwaysdown action) or ended up in local minimum of action backto  starting  state.  Simulations  resulted  that  mainly  discountfactor  was  cause  of  a  bad  symptom,  as  seen  in  Figure  12.This  behaviour  can  correspond  to  disease  1  since  the  agentgot  stuck  in  an  local  maximum  behaviour  even  though  ifit  found  large  rewards  a  few  times.  Also  corresponding  todisease 1 in maze environment, a low discount factor seemsto be the cause of this learning behaviour.IV.  DISCUSSIONAs  seen  in  results  its  possible  to  simulate  at  least  threediseases  with  a  simple  RL  model  in  a  simple  environment.The  fictitious  disease  1  with  symptoms  of  getting  stuck  inlocal maximum was mainly caused by a low discount factor.It  would  be  interesting  to  see  if  there  are  any  connectionsto  the  fictitious  disease  1  and  diseases  like  drug  addiction,which  is  linked  to  learning.  If  there  is,  one  can  use  theknowledge that the discount factor is cause to such symptomsFig. 8.Example two of an agent, in maze, with disease 2 also showing aslow learning behaviour but with different parameters.Fig.  9.Last  example  of  an  agent  with  disease  2,  also  showing  to  be  aslow learner.to diagnosis and treatment.For  disease  2,  where  the  agent  had  a  slow  learning  be-haviour, the results show that one symptom can occur due tomany  different  parameter  settings.  This  groups  agents  withsymptoms from disease 2 into several different groups. Onemight point that need different treatment depending on whichparameter  is  causing  the  symptoms  might  be  important.Since  learning  rate  and  discount  factor  might  be  linked  todifferent  brain  mechanisms,  one  need  to  treat  these  sickagents  different.  Else,  lowering  discount  factor  in  an  agentwith  disease,  as  seen  in  Figure  7,  might  lead  to  symptomsas disease 1 which would be a side effect of the treatment.The third disease was the most severe, resulting in an agentthat could not find a reward over 400 episodes. If treatmentwas  to  adjust  learning  rate  and  discount  factor  in  the  rightway,  the  agent  should  be  able  to  find  an  optimal  learningbehaviour.  This  is  showing  that  treatment  of  underlyingparameter  is  of  great  importance  even  if  the  symptoms  isthe same.One  could  argue  that  these  simulations  are  very  simplifiedand not of practical use. As described above, the motivationwas  to  see  and  simulate  symptoms  and  diseases  that  corre-spond to different parameters, which was successfully done.
Fig. 10.    An agent with disease 3 causes the agent not be able to solve themaze and not find a single reward in 400 episodes.Fig. 11.A normal agent finds the optimal behaviour after a few episodeswith learning rate= 0.9and discount factor= 0.8.The  model  and  simulations  where  to  show  how  parameterscan affect symptoms and as now shown, one can implementa more advanced model for future work. Another interestingaspect to look at is a model for perception where the agentdid not have full observability. This would make it possible tosee similarities to diseases where the perception is abnormal.To  summarise  this  project,  it  have  been  shown  that  with  aQ-learning model for an agent’s learning behaviour, we cansimulate different diseases. With simulations, the parameterswere shown to have great importance for resulting symptomsand there are several symptoms of diseases that occur whenparameters  are  abnormal.  We  can  show  that  a  symptom  ofdisease can be caused of several different parameter changes,even with a simple model in a simple environment.