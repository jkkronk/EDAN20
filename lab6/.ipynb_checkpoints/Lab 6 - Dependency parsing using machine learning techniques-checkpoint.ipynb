{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6 - Dependency parsing using machine learning techniques\n",
    "Laboration 6 in EDAN20 @ LTH - http://cs.lth.se/edan20/coursework/assignment-6/\n",
    "\n",
    "Author: Jonatan Kronander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The objectives of this assignment are to:\n",
    "* Extract feature vectors and train a classifier\n",
    "* Write a statistical dependency parser\n",
    "* Understand how to design parameter sets\n",
    "* Write a short report on your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the corpus and evaluating the results\n",
    "First load model data from lab 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_feature1.pickle', 'rb') as handle:\n",
    "    X1 = pickle.load(handle)\n",
    "\n",
    "with open('Y_feature1.pickle', 'rb') as handle:\n",
    "    Y1 = pickle.load(handle)\n",
    "\n",
    "with open('X_feature2.pickle', 'rb') as handle:\n",
    "    X2 = pickle.load(handle)\n",
    "    \n",
    "with open('Y_feature2.pickle', 'rb') as handle:\n",
    "    Y2 = pickle.load(handle)\n",
    "    \n",
    "with open('X_feature3.pickle', 'rb') as handle:\n",
    "    X3 = pickle.load(handle)\n",
    "    \n",
    "with open('Y_feature2.pickle', 'rb') as handle:\n",
    "    Y3 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_1 = ['pos_stack_0', 'word_stack_0',\n",
    "                    'pos_queue_0','word_queue_0',\n",
    "                    'can-re', 'can-la']\n",
    "\n",
    "feature_names_2 = ['pos_stack_0', 'pos_stack_1', 'word_stack_0', 'word_stack_1',\n",
    "                    'pos_queue_0', 'pos_queue_1', 'word_queue_0', 'word_queue_1',\n",
    "                    'can-re', 'can-la']\n",
    "\n",
    "feature_names_3 = ['pos_stack_0', 'pos_stack_1', 'word_stack_0', 'word_stack_1',\n",
    "                    'pos_queue_0', 'pos_queue_1', 'word_queue_0', 'word_queue_1', \n",
    "                    'pos_next_stack', 'word_next_stack',\n",
    "                    'pos_next_queue', 'pos_next_queue',\n",
    "                    'can-re', 'can-la']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dict(X, feature_names):\n",
    "    \"\"\"\n",
    "    Creates a feature dict from feature names and feature list\n",
    "    :param: feature list\n",
    "    :param: feature names\n",
    "    :return: dict\n",
    "    \"\"\"\n",
    "    X_train = []\n",
    "    for el in X:\n",
    "        X_dict = {key: el[i] for i,key in enumerate(feature_names)}\n",
    "        X_train.append(X_dict)\n",
    "        \n",
    "    return X_train\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_dict = create_feature_dict(X1, feature_names_1)\n",
    "X2_dict = create_feature_dict(X2, feature_names_2)\n",
    "X3_dict = create_feature_dict(X3, feature_names_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with features 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n",
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X1_dict)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model1 = classifier.fit(X, Y1)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with features 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n",
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X2_dict)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model2 = classifier.fit(X, Y2)\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with features 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n",
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X3_dict)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model3 = classifier.fit(X, Y3)\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we want to embed the data in Nivre's parser to compute their respective efficiencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transition\n",
    "import conll\n",
    "import dparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "\n",
    "sentences_train = conll.read_sentences(\"train.conll\")\n",
    "formatted_corpus_train = conll.split_rows(sentences_train, column_names_2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_2006 = ['id', 'form', 'lemma', 'cpostag', 'postag', 'feats', 'head', 'deprel', 'phead', 'pdeprel']\n",
    "\n",
    "sentences_test = conll.read_sentences(\"test_blind.conll\")\n",
    "formatted_corpus_test = conll.split_rows(sentences_test, column_names_2006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### We proceed sentence by sentence, and word by word. For a certain state, it will predict the next action using our classifier. We will then execute the corresponding action: la, ra, re, or sh. If an action is not possible, we will carry out a shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
