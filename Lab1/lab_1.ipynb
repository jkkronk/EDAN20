{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laboration 1 in EDAN20 @ LTH - http://cs.lth.se/edan20/coursework/assignment-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objectives of this assignment are to:\n",
    "\n",
    "-Write a program that collects all the words from a set of documents\n",
    "\n",
    "-Build an index from the words\n",
    "\n",
    "-Know what indexing is\n",
    "\n",
    "-Represent a document using the Tf.Idf value\n",
    "\n",
    "-Write a short report of 1 to 2 pages on the assignment\n",
    "\n",
    "-Read a short text on an industrial system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The index file will contain all the unique words in the document, where each word is associated with the list of its positions in the document.\n",
    "\n",
    "* You will represent this index as a dictionary where the keys will be the words and the values, the lists of positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As words, you will consider all the strings of letters that you will set in lower case. You will not index the rest (i.e. numbers or symbols).\n",
    "\n",
    "This is done by first using function txtClean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def txtClean(text):\n",
    "    \"\"\"\n",
    "    Replace capital characters to small characters\n",
    "    \n",
    "    Input txt file\n",
    "    Output txt file\n",
    "    \"\"\"\n",
    "    # Remove new lines\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    \n",
    "    # Replace [A-Ö] with [a-ö]\n",
    "    def toLowercase(matchobj):\n",
    "        return matchobj.group(1).lower()\n",
    "    \n",
    "    text = re.sub(r'([A-Z])', toLowercase, text)\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"Selma/bannlyst.txt\").read()  \n",
    "txt = txtClean(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To extract the words, you will use Unicode regular expressions. Do not use \\w+, for instance, but the Unicode equivalent. The word positions will correspond to the number of characters from the beginning of the file. (The word offset from the beginning)\n",
    "\n",
    "* You will use finditer() to find the positions of the words. This will return you match objects, where you will get the matches and the positions with the group() and start() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2dict(text,originaltext):\n",
    "    \"\"\"\n",
    "    Creates a dict with (word:list[index apperences]) from input string\n",
    "    \n",
    "    Input string, string\n",
    "    Output dict\n",
    "    \"\"\"\n",
    "    stringList = re.findall(r\"[a-zåäö]+\",text) #This finds all words from a txt file. r\"[a-zåäö]+ equal to r\"\\w+\" \n",
    "\n",
    "    stringDict = dict.fromkeys(stringList) #Creates dict (and remove doublicates)\n",
    "\n",
    "    for word in stringDict:\n",
    "        wordIndices = []\n",
    "        pattern = r\"\\b\"+word+ r\"\\b\" #Only look at word \n",
    "        \n",
    "        for m in re.finditer(pattern, originaltext): #Iterate thorugh every word\n",
    "            wordIndices.append(m.start(0))\n",
    "            \n",
    "        stringDict.update({word:wordIndices})\n",
    "    return stringDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtDict = string2dict(txt,text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7950"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txtDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with bannlyst text. The word gjord occurs three times in the text at positions 8551, 183692, and 220875, uppklarnande, once at position 8567, and stjärnor, once at position 8590. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8551, 183692, 220875]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDict['gjord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8567]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDict['uppklarnande']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8590]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDict['stjärnor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You will use the pickle package to write your dictionary in an file, see https://wiki.python.org/moin/UsingPickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('BannlystTxtDict.pickle', 'wb') as handle:\n",
    "    pickle.dump(txtDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BannlystTxtDict.pickle', 'rb') as handle:\n",
    "    BannlystTxtDict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BannlystTxtDict == txtDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the content of a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that reads all the files in a folder with a specific suffix (txt). You will need the Python os package, see https://docs.python.org/3/library/os.html. You will return the file names in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(fileDir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param filedir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(fileDir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(\"selma\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['troll.txt',\n",
       " 'kejsaren.txt',\n",
       " 'marbacka.txt',\n",
       " 'herrgard.txt',\n",
       " 'nils.txt',\n",
       " 'osynliga.txt',\n",
       " 'jerusalem.txt',\n",
       " 'bannlyst.txt',\n",
       " 'gosta.txt']"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a master index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete your program with the creation of master index, where you will associate each word of the corpus with the files, where it occur and its positions. (a posting list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def addAll(fileDir,files): \n",
    "    \"\"\"\n",
    "    Reads all files in list and matches to txt files\n",
    "    :param dict:\n",
    "    :param dir:\n",
    "    :param files:\n",
    "    :return dict:\n",
    "    \"\"\"\n",
    "    allDict = {}\n",
    "\n",
    "    for file in files:\n",
    "        text = open(fileDir+\"/\"+file).read()  \n",
    "        txt = txtClean(text)\n",
    "        stringList = re.findall(r\"[a-zåäö]+\",text)\n",
    "        allDict.update(dict.fromkeys(stringList))\n",
    "    \n",
    "    for word in allDict: # Iterate through every word in dict\n",
    "        wordIndices = []\n",
    "        pattern = r\"\\b\"+word+ r\"\\b\" # Only look at word\n",
    "        fileDict = dict.fromkeys(files)\n",
    "        \n",
    "        for file in files: # Iterate through every text\n",
    "            text = open(fileDir+\"/\"+file).read()  \n",
    "            for m in re.finditer(pattern, text): #Iterate through every word in text\n",
    "                wordIndices.append(m.start(0))\n",
    "            fileDict.update({file:wordIndices})\n",
    "            \n",
    "        allDict.update({word:fileDict})\n",
    "    \n",
    "    return retDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDict = addAll('selma',files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('masterDict.pickle', 'wb') as handle:\n",
    "    pickle.dump(masterDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an except of the master index with the words samlar and ände:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'samlar': {'nils.txt': [53499, 120336], 'gosta.txt': [317119, 414300, 543686],'osynliga.txt': [410995, 871322]},\n",
    "\n",
    "'ände':{'nils.txt': [3991],'kejsaren.txt': [51100],'marbacka.txt': [374231],'troll.txt': [39726],'osynliga.txt': [742747]},"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word samlar, for instance, occurs three times in the gosta text at positions 317119, 414300, and 543686."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDict[\"samlar\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDict[\"ände\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
