{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDAN20 Laboration 1 - 12/9\n",
    "### Author: Sepehr Tayari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the laboration is to create a program which reads one file, 'file_name.txt' and outputs an index file 'file_name.idx'.\n",
    "\n",
    "In addition to this, the program should also create different dictionaries. One dictionary which saves words as keys, and list of index position of each word as value. A master index shall also be created, which has words as key, and another dictionary as value. The second dictionary should have file name as key and list of word position as value. The master indexer should represent data of in which text files a certain word appears in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing all modules we need, and creating the dictionaries which will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import pickle\n",
    "import os\n",
    "import math.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "word_dict = dict()\n",
    "book_dict = dict()\n",
    "master_dict = dict()\n",
    "corpus = dict()\n",
    "word_count = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data set of all words are saved in dictionary corpus. The dict is built, and the text is cleaned away from characters which are not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \"\"\"\n",
    "    :param str text: File to be cleaned up. The function will remove all non letter characters and make all letters\n",
    "    lower case\n",
    "    :return str text: the new text file\n",
    "    \"\"\"\n",
    "    # Removes all new lines and replaces with spaces.\n",
    "    text = re.sub(\"\\n\", \" \", text)\n",
    "    # Replace non-letters with nothing\n",
    "    text = re.sub('\\P{L}', '', text)\n",
    "    # Make all words lower case\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dict(text):\n",
    "    \"\"\"\n",
    "    Adds words to global dictionary 'corpus'\n",
    "    :param str text: all words in the string text will be added to a dictionary. String text must be letters only and\n",
    "    all lower case.\n",
    "    \"\"\"\n",
    "    global corpus\n",
    "    text = text_cleaner(text)\n",
    "    for word in text.split(' '):\n",
    "        corpus[word] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read in our needed files we use the following method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(dir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param dir: The directory of the files\n",
    "    :param suffix: The suffix the files end in. Example '.txt'.\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index all the files, the method word_indexer is used, which takes one file as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_indexer(text):\n",
    "    \"\"\"\n",
    "    Indexes all the words in the text, and appends them to the global dictionay word_dict.\n",
    "    :param str text: the string which should be indexed.\n",
    "    \"\"\"\n",
    "    # Match every word in string text.\n",
    "    for match in re.finditer(r'\\p{L}+', text.lower()):\n",
    "        match_word = match.group()\n",
    "        # If the word exist in the dict, append the new index.\n",
    "        if match_word in word_dict.keys():\n",
    "            word_dict[match_word].append(match.start(0))\n",
    "        # If the word does not yet exist as a key. Add it with its index for the first time, as a list.\n",
    "        else:\n",
    "            word_dict[match_word] = [match.start(0)]\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To index all words to their corresponding file, in the master index, following method is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def master_indexer():\n",
    "    \"\"\"\n",
    "    Indexes all words to which textfile they occur in, and at which index.\n",
    "    {word: {file_name: [start0, start1, ...]}} to file master_dict.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    global master_dict\n",
    "    global corpus\n",
    "    for word in corpus:\n",
    "        master_dict[word] = {}\n",
    "        for file_name in files:\n",
    "            try:\n",
    "                master_dict[word][file_name] = book_dict[file_name][word]\n",
    "            except:\n",
    "                nop = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files('Selma2', 'txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in files:\n",
    "\n",
    "    fil = open('Selma2/'+file_name, 'r')\n",
    "    fil = fil.read()\n",
    "\n",
    "    # We also do a word count for each file. This will be needed later in\n",
    "    # the tf-idf\n",
    "    word_count[file_name]= len(fil.split(\" \"))\n",
    "\n",
    "\n",
    "    word_dict = word_indexer(fil)\n",
    "    build_dict(fil)\n",
    "    # pickle.dump(word_dict, open('{}.idx'.format(file_name), 'wb'))\n",
    "    # Building master index:\n",
    "    book_dict[file_name] = word_dict\n",
    "\n",
    "    # Reset the word_dict so it can store new text file\n",
    "    word_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example as seen on the course website. Start index of word 'gjord' in file 'bannlyst.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8551, 183692, 220875]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dict['bannlyst.txt']['gjord']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the master index, and look for the word 'samlar'. We get the same start index as given on the course webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nils.txt': [53499, 120336],\n",
       " 'osynliga.txt': [410995, 871322],\n",
       " 'gosta.txt': [317119, 414300, 543686]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_indexer()\n",
    "master_dict['samlar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representing Documents with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By representing the documents with tf-idf, and look at cosinus similarities we can determine how similar the different texts are, with no regard of the order of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(files):\n",
    "    \"\"\"\n",
    "    Calculates the tf idf for the gives files.\n",
    "    :param files:\n",
    "    :return: dict tf_dict:\n",
    "    \"\"\"\n",
    "    n = 9\n",
    "    global word_count\n",
    "    tf_dict = dict()\n",
    "    df = 0\n",
    "    for file_name in files:  # In each file, how many times do the word occur\n",
    "        tf_dict[file_name] = dict()\n",
    "        for word in master_dict:\n",
    "            df = len(master_dict[word]) # In how many stories does the word occur\n",
    "            try:\n",
    "                tf = len(master_dict[word][file_name])\n",
    "            except:\n",
    "                nop = 1\n",
    "            if df != 0:\n",
    "                weigh = math.log10(9/df)*tf/word_count[file_name]\n",
    "                tf_dict[file_name][word] = weigh\n",
    "                df = 0\n",
    "                tf = 0\n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run our files through the tf_idf function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict = tf_idf(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now got the tf-idf values. And wish to create a matrix with the cosinus similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cos_sim_matrix(tf_dict):\n",
    "    \"\"\"\n",
    "\n",
    "    :param tf_dict: dictionary with {file_name: {word : tf-value}}\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    doc_matrix = np.zeros((9,len(corpus.keys())))\n",
    "    word_list = corpus.keys()\n",
    "    file_list = dict_dict.keys()\n",
    "    for i, word in enumerate(word_list):\n",
    "        for j, file in enumerate(file_list):\n",
    "            try:\n",
    "                doc_matrix[j, i] =tf_dict[file][word]# tf_dict[file][word]\n",
    "                #print(doc_matrix[j, i], 'hej', tf_dict[file][word])\n",
    "            except:\n",
    "                #print('didnt work')\n",
    "                nop = 1\n",
    "    df = pd.DataFrame(doc_matrix)\n",
    "    similarity_matrix = cosine_similarity(df)\n",
    "    return similarity_matrix\n",
    "    # np.ndarray.round(similarity_matrix, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the two most similar files, we must look at the matrix and find the maximum value, except for the diagonal (since each file is identical to itself). For better visualisation I will round the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.088 0.025 0.004 0.019 0.028 0.007 0.007 0.033]\n",
      " [0.088 1.    0.046 0.001 0.004 0.006 0.002 0.002 0.007]\n",
      " [0.025 0.046 1.    0.012 0.042 0.044 0.014 0.005 0.03 ]\n",
      " [0.004 0.001 0.012 1.    0.014 0.021 0.008 0.001 0.015]\n",
      " [0.019 0.004 0.042 0.014 1.    0.032 0.013 0.005 0.025]\n",
      " [0.028 0.006 0.044 0.021 0.032 1.    0.042 0.006 0.056]\n",
      " [0.007 0.002 0.014 0.008 0.013 0.042 1.    0.007 0.009]\n",
      " [0.007 0.002 0.005 0.001 0.005 0.006 0.007 1.    0.005]\n",
      " [0.033 0.007 0.03  0.015 0.025 0.056 0.009 0.005 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "sim_matrix = np.ndarray.round(build_cos_sim_matrix(tf_dict), 3)\n",
    "print(sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix is symetrical around the diagonal. We only need to inspect one of the upper or lower halfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = np.triu(sim_matrix, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.088 0.025 0.004 0.019 0.028 0.007 0.007 0.033]\n",
      " [0.    0.    0.046 0.001 0.004 0.006 0.002 0.002 0.007]\n",
      " [0.    0.    0.    0.012 0.042 0.044 0.014 0.005 0.03 ]\n",
      " [0.    0.    0.    0.    0.014 0.021 0.008 0.001 0.015]\n",
      " [0.    0.    0.    0.    0.    0.032 0.013 0.005 0.025]\n",
      " [0.    0.    0.    0.    0.    0.    0.042 0.006 0.056]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.007 0.009]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.005]\n",
      " [0.    0.    0.    0.    0.    0.    0.    0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value is 0.088 and corresponds to coordinates (array([0]), array([1]))\n"
     ]
    }
   ],
   "source": [
    "print('Maximum value is {} and corresponds to coordinates {}'.format(np.amax(upper), np.where(upper==np.amax(upper))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The index has the same index as the order of dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 -> troll.txt\n",
      "Index 1 -> kejsaren.txt\n",
      "Index 2 -> marbacka.txt\n",
      "Index 3 -> herrgard.txt\n",
      "Index 4 -> nils.txt\n",
      "Index 5 -> osynliga.txt\n",
      "Index 6 -> jerusalem.txt\n",
      "Index 7 -> bannlyst.txt\n",
      "Index 8 -> gosta.txt\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(dict_dict):\n",
    "    print('Index {} -> {}'.format(i, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hej' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-32a0617aab4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhej\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hej' is not defined"
     ]
    }
   ],
   "source": [
    "hej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
