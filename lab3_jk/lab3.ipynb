{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab 3 - Extracting noun groups using machine learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The objectives of this assignment are to:\n",
    "\n",
    "- Write a program to detect partial syntactic structures\n",
    "- Understand the principles of supervised machine learning techniques applied to language processing\n",
    "- Use a popular machine learning toolkit: scikit-learn\n",
    "- Write a short report of 1 to 2 pages on the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a training and a test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "b_train_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt\").read() # Open file and read\n",
    "train_text = str(b_train_text,'utf-8')\n",
    "b_test_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt\").read() # Open file and read\n",
    "test_text = str(b_test_text,'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEXT EXAMPLE TRAIN---\n",
      " Confidence NN B-NP\n",
      "in IN B-PP\n",
      "the DT B-NP\n",
      "pound NN I-NP\n",
      "is VBZ B-VP\n",
      "widely RB I-VP\n",
      "expected VBN I-VP\n",
      "to TO I-VP\n",
      "take VB I-VP\n",
      "another DT B-NP\n",
      "sharp JJ I-NP\n",
      "dive NN I-NP\n",
      "if IN B-SBAR\n",
      "trade NN B-NP\n",
      "figur \n",
      " ---TEXT EXAMPLE TEST--- \n",
      " Rockwell NNP B-NP\n",
      "International NNP I-NP\n",
      "Corp. NNP I-NP\n",
      "'s POS B-NP\n",
      "Tulsa NNP I-NP\n",
      "unit NN I-NP\n",
      "said VBD B-VP\n",
      "it PRP B-NP\n",
      "signed VBD B-VP\n",
      "a DT B-NP\n",
      "tentative JJ I-NP\n",
      "agreement NN I-NP\n",
      "extending VBG B-\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEXT EXAMPLE TRAIN---\\n\",train_text[:200], \"\\n ---TEXT EXAMPLE TEST--- \\n\",test_text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most statistical algorithms for language processing start with a so-called baseline. The baseline figure corresponds to the application of a minimal technique that is used to assess the difficulty of a task and for comparison with further programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the baseline proposed by the organizers of the CoNLL 2000 shared task (In the Results Sect.). What do you think of it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They get pretty high score overall but no method applying advanced ml methods with deep neural networks. (Which is understandable since the conference was held at year 2000.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement this baseline program. You may either create a completely new program or start from an existing program that you will modify. https://github.com/pnugues/ilppp/tree/master/programs/labs/chunking/chunker_python/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the train function so that it computes the chunk distribution for each part of speech. You will use the train file to derive your distribution and you will store the results in a dictionary. Below, you have an excerpt of the expected results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['form', 'pos', 'chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.read_sentences(train_text)\n",
    "sentences_train = train_text.split('\\n\\n') \n",
    "#sentences_train.remove(sentences_train[-1]) # Last element needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.split_rows(train_corpus, column_names)\n",
    "train_corpus = []\n",
    "for sentence in sentences_train:\n",
    "    rows = sentence.split('\\n')\n",
    "    sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "    train_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.read_sentences(train_text)\n",
    "sentences_test = test_text.split('\\n\\n')\n",
    "#sentences_test.remove(sentences_test[-1]) # Last element needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_corpus = conll_reader.split_rows(train_corpus, column_names)\n",
    "test_corpus = []\n",
    "for sentence in sentences_test:\n",
    "    rows = sentence.split('\\n')\n",
    "    sentence = [dict(zip(column_names, row.split())) for row in rows]\n",
    "    test_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'form': 'Chancellor', 'pos': 'NNP', 'chunk': 'O'},\n",
       " {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'Exchequer', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       " {'form': 'Nigel', 'pos': 'NNP', 'chunk': 'B-NP'},\n",
       " {'form': 'Lawson', 'pos': 'NNP', 'chunk': 'I-NP'},\n",
       " {'form': \"'s\", 'pos': 'POS', 'chunk': 'B-NP'},\n",
       " {'form': 'restated', 'pos': 'VBN', 'chunk': 'I-NP'},\n",
       " {'form': 'commitment', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'B-PP'},\n",
       " {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'firm', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'monetary', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'policy', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'has', 'pos': 'VBZ', 'chunk': 'B-VP'},\n",
       " {'form': 'helped', 'pos': 'VBN', 'chunk': 'I-VP'},\n",
       " {'form': 'to', 'pos': 'TO', 'chunk': 'I-VP'},\n",
       " {'form': 'prevent', 'pos': 'VB', 'chunk': 'I-VP'},\n",
       " {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'freefall', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'sterling', 'pos': 'NN', 'chunk': 'B-NP'},\n",
       " {'form': 'over', 'pos': 'IN', 'chunk': 'B-PP'},\n",
       " {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP'},\n",
       " {'form': 'past', 'pos': 'JJ', 'chunk': 'I-NP'},\n",
       " {'form': 'week', 'pos': 'NN', 'chunk': 'I-NP'},\n",
       " {'form': '.', 'pos': '.', 'chunk': 'O'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(corpus):\n",
    "    \"\"\"\n",
    "    Computes the part-of-speech distribution\n",
    "    in a CoNLL 2000 file\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = {}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['pos'] in pos_cnt:\n",
    "                pos_cnt[row['pos']] += 1\n",
    "            else:\n",
    "                pos_cnt[row['pos']] = 1\n",
    "    return pos_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(corpus):\n",
    "    \"\"\"\n",
    "    Computes the chunk distribution by pos\n",
    "    The result is stored in a dictionary\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pos_cnt = count_pos(corpus)\n",
    "\n",
    "    # We compute the chunk distribution by POS\n",
    "    \"\"\"\n",
    "    Fill in code to compute the chunk distribution for each part of speech\n",
    "    \"\"\"\n",
    "    chunk_dist = {key: {} for key in pos_cnt.keys()}\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if row['chunk'] in chunk_dist[row['pos']]:\n",
    "                chunk_dist[row['pos']][row['chunk']] += 1\n",
    "            else:\n",
    "                chunk_dist[row['pos']][row['chunk']] = 1\n",
    "        \n",
    "    print(\"Example of probdist for JJR: \", chunk_dist['JJR'])\n",
    "    # We determine the best association\n",
    "    \"\"\"\n",
    "    Fill in code so that for each part of speech, you select the most frequent chunk.\n",
    "    You will build a dictionary with key values:\n",
    "    pos_chunk[pos] = most frequent chunk for pos\n",
    "    \"\"\"\n",
    "    pos_ret = {key: \"\" for key in pos_cnt.keys()}\n",
    "    for pos in chunk_dist:\n",
    "        max_value = 0\n",
    "        max_chunk = \"\"\n",
    "        for chunk in chunk_dist[pos]:\n",
    "            if max_value < chunk_dist[pos][chunk]:\n",
    "                max_value = chunk_dist[pos][chunk]\n",
    "                max_chunk = chunk\n",
    "        pos_ret[pos] = max_chunk\n",
    "    \n",
    "    return pos_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of probdist for JJR:  {'B-NP': 382, 'B-ADJP': 111, 'I-ADJP': 45, 'B-ADVP': 63, 'I-ADVP': 17, 'B-VP': 2, 'I-NP': 204, 'I-VP': 11, 'O': 16, 'B-PP': 2}\n",
      "Example of train model for NN:  I-NP\n"
     ]
    }
   ],
   "source": [
    "model = train(train_corpus)\n",
    "\n",
    "print(\"Example of train model for NN: \",model['NN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, corpus):\n",
    "    \"\"\"\n",
    "    Predicts the chunk from the part of speech\n",
    "    Adds a pchunk column\n",
    "    :param model:\n",
    "    :param corpus:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    We add a predicted chunk column: pchunk\n",
    "    \"\"\"\n",
    "    for sentence in corpus:\n",
    "        for row in sentence:\n",
    "            if 'pos' in row:\n",
    "                row['pchunk'] = model[row['pos']]\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'form': 'In', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'}, {'form': 'the', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'same', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'statement', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': ',', 'pos': ',', 'chunk': 'O', 'pchunk': 'O'}, {'form': 'US', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'Facilities', 'pos': 'NNPS', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'also', 'pos': 'RB', 'chunk': 'B-ADVP', 'pchunk': 'B-ADVP'}, {'form': 'said', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'}, {'form': 'it', 'pos': 'PRP', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'had', 'pos': 'VBD', 'chunk': 'B-VP', 'pchunk': 'B-VP'}, {'form': 'bought', 'pos': 'VBN', 'chunk': 'I-VP', 'pchunk': 'I-VP'}, {'form': 'back', 'pos': 'RB', 'chunk': 'B-ADVP', 'pchunk': 'B-ADVP'}, {'form': '112,000', 'pos': 'CD', 'chunk': 'B-NP', 'pchunk': 'I-NP'}, {'form': 'of', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'}, {'form': 'its', 'pos': 'PRP$', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'common', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'shares', 'pos': 'NNS', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'in', 'pos': 'IN', 'chunk': 'B-PP', 'pchunk': 'B-PP'}, {'form': 'a', 'pos': 'DT', 'chunk': 'B-NP', 'pchunk': 'B-NP'}, {'form': 'private', 'pos': 'JJ', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': 'transaction', 'pos': 'NN', 'chunk': 'I-NP', 'pchunk': 'I-NP'}, {'form': '.', 'pos': '.', 'chunk': 'O', 'pchunk': 'O'}]\n"
     ]
    }
   ],
   "source": [
    "predicted = predict(model, test_corpus)\n",
    "\n",
    "print(predicted[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(predicted):\n",
    "    \"\"\"\n",
    "    Evaluates the predicted chunk accuracy\n",
    "    :param predicted:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    word_cnt = 0\n",
    "    correct = 0\n",
    "    for sentence in predicted:\n",
    "        for row in sentence:\n",
    "            word_cnt += 1\n",
    "            if row['chunk'] == row['pchunk']:\n",
    "                correct += 1\n",
    "    return correct / word_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7729066846782194\n"
     ]
    }
   ],
   "source": [
    "accuracy = eval(predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_out = open('out', 'w')\n",
    "    # We write the word (form), part of speech (pos),\n",
    "    # gold-standard chunk (chunk), and predicted chunk (pchunk)\n",
    "for sentence in predicted:\n",
    "    for row in sentence:\n",
    "        f_out.write(row['form'] + ' ' + row['pos'] + ' ' + row['chunk'] + ' ' + row['pchunk'] + '\\n')\n",
    "    f_out.write('\\n')\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perl conlleval.txt <out\n",
      "processed 47377 tokens with 23852 phrases; found: 26992 phrases; correct: 19592.\n",
      "accuracy:  77.29%; precision:  72.58%; recall:  82.14%; FB1:  77.07\n",
      "             ADJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             ADVP: precision:  44.33%; recall:  77.71%; FB1:  56.46  1518\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:  50.00%; recall:  50.00%; FB1:  50.00  2\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  79.87%; recall:  86.80%; FB1:  83.19  13500\n",
      "               PP: precision:  74.73%; recall:  97.07%; FB1:  84.45  6249\n",
      "              PRT: precision:  75.00%; recall:   8.49%; FB1:  15.25  12\n",
      "             SBAR: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               VP: precision:  60.53%; recall:  74.22%; FB1:  66.68  5711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd_1 = \"perl conlleval.txt <out\"\n",
    "print(cmd_1)\n",
    "\n",
    "p = subprocess.Popen(cmd_1, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will apply and extend the ml_chunker.py program. You will start from the original program you downloaded and modify it so that you understand how to improve the performance of your chunker. You will not add new features to the feature vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_train_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt\").read() # Open file and read\n",
    "train_text = str(b_train_text,'utf-8')\n",
    "train_text = train_text.strip()\n",
    "train_sentences = train_text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = \"BOS BOS BOS\\n\"\n",
    "    end = \"\\nEOS EOS EOS\"\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    sentence = start + sentence\n",
    "    sentence += end\n",
    "\n",
    "    # Each sentence is a list of rows\n",
    "    sentence = sentence.splitlines()\n",
    "    padded_sentence = list()\n",
    "    for line in sentence:\n",
    "        line = line.split()\n",
    "        padded_sentence.append(line)\n",
    "    # print(padded_sentence)\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][0].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][1])\n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            feature_line.append(padded_sentence[i + j][2])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size][2])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the features...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2','pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']\n",
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "   \n",
    "print(\"Extracting the features...\")\n",
    "\n",
    "X_dict, y = extract_features(train_sentences, w_size, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding the features...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)\n",
    "# The statement below will swallow a considerable memory\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model = classifier.fit(X, y)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt\").read() # Open file and read\n",
    "test_text = str(b_test_text,'utf-8')\n",
    "test_text = test_text.strip()\n",
    "test_sentences = test_text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we carry out a chunk tag prediction and we report the per tag error.\n",
    "\n",
    "This is done for the whole corpus without regard for the sentence structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the chunks in the test set...\n",
      "Classification report for classifier LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       0.83      0.67      0.74       438\n",
      "     B-ADVP       0.81      0.81      0.81       866\n",
      "    B-CONJP       0.67      0.44      0.53         9\n",
      "     B-INTJ       1.00      0.50      0.67         2\n",
      "      B-LST       0.00      0.00      0.00         5\n",
      "       B-NP       0.96      0.96      0.96     12422\n",
      "       B-PP       0.96      0.98      0.97      4811\n",
      "      B-PRT       0.77      0.74      0.75       106\n",
      "     B-SBAR       0.89      0.84      0.87       535\n",
      "       B-VP       0.95      0.95      0.95      4658\n",
      "     I-ADJP       0.86      0.54      0.66       167\n",
      "     I-ADVP       0.63      0.48      0.55        89\n",
      "    I-CONJP       0.77      0.77      0.77        13\n",
      "      I-LST       0.00      0.00      0.00         2\n",
      "       I-NP       0.96      0.96      0.96     14376\n",
      "       I-PP       0.88      0.58      0.70        48\n",
      "     I-SBAR       0.07      0.25      0.11         4\n",
      "       I-VP       0.93      0.95      0.94      2646\n",
      "          O       0.95      0.96      0.96      6180\n",
      "\n",
      "avg / total       0.95      0.95      0.95     47377\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Predicting the chunks in the test set...\")\n",
    "X_test_dict, y_test = extract_features(test_sentences, w_size, feature_names)\n",
    "\n",
    "# Vectorize the test set and one-hot encoding\n",
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_test_predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tag the test set and we save it.\n",
    "\n",
    "This prediction is redundant with the piece of code above, but we need to predict one sentence at a time to have the same corpus structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ml(test_sentences, feature_names, f_out):\n",
    "    for test_sentence in test_sentences:\n",
    "        X_test_dict, y_test = extract_features_sent(test_sentence, w_size, feature_names)\n",
    "        # Vectorize the test sentence and one hot encoding\n",
    "        X_test = vec.transform(X_test_dict)\n",
    "        # Predicts the chunks and returns numbers\n",
    "        y_test_predicted = classifier.predict(X_test)\n",
    "        # Appends the predicted chunks as a last column and saves the rows\n",
    "        rows = test_sentence.splitlines()\n",
    "        rows = [rows[i] + ' ' + y_test_predicted[i] for i in range(len(rows))]\n",
    "        for row in rows:\n",
    "            f_out.write(row + '\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting the test set...\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting the test set...\")\n",
    "f_out = open('outml', 'w')\n",
    "predict_ml(test_sentences, feature_names, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perl conlleval.txt <outml\n",
      "processed 47377 tokens with 23852 phrases; found: 24251 phrases; correct: 22010.\n",
      "accuracy:  94.96%; precision:  90.76%; recall:  92.28%; FB1:  91.51\n",
      "             ADJP: precision:  74.22%; recall:  65.07%; FB1:  69.34  384\n",
      "             ADVP: precision:  78.45%; recall:  79.45%; FB1:  78.94  877\n",
      "            CONJP: precision:  44.44%; recall:  44.44%; FB1:  44.44  9\n",
      "             INTJ: precision: 100.00%; recall:  50.00%; FB1:  66.67  1\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  90.31%; recall:  92.34%; FB1:  91.31  12701\n",
      "               PP: precision:  95.87%; recall:  97.86%; FB1:  96.85  4911\n",
      "              PRT: precision:  77.23%; recall:  73.58%; FB1:  75.36  101\n",
      "             SBAR: precision:  89.15%; recall:  84.49%; FB1:  86.76  507\n",
      "               VP: precision:  90.84%; recall:  92.83%; FB1:  91.82  4760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmd_2 = \"perl conlleval.txt <outml\"\n",
    "print(cmd_2)\n",
    "\n",
    "p = subprocess.Popen(cmd_2, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the feature vector that corresponds to the ml_chunker.py program? Is it the same Kudoh and Matsumoto used in their experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vector in ml_chunker is similar but not exactly the same. Missing features: The values of the two previous chunk tags in the first part of the window: c i-2 , c i-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the performance of the chunker?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above, Accuracy 94.96%, compared with 77.29% when we used Maximum likelihood method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove the lexical features (the words) from the feature vector and measure the performance. You should observe a decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_re(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence: string containing the CoNLL structure of a sentence\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = \"BOS BOS BOS\\n\"\n",
    "    end = \"\\nEOS EOS EOS\"\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    sentence = start + sentence\n",
    "    sentence += end\n",
    "\n",
    "    # Each sentence is a list of rows\n",
    "    sentence = sentence.splitlines()\n",
    "    padded_sentence = list()\n",
    "    for line in sentence:\n",
    "        line = line.split()\n",
    "        padded_sentence.append(line)\n",
    "    # print(padded_sentence)\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        \"\"\"\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][0].lower())\n",
    "        \"\"\"\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][1])\n",
    "        # The chunks (Up to the word)\n",
    "        \"\"\"\n",
    "        for j in range(w_size):\n",
    "            X.append(padded_sentence[i + j][2])\n",
    "        \"\"\"\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size][2])\n",
    "    return X, y\n",
    "\n",
    "def extract_features_re(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_re(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l\n",
    "\n",
    "def predict_ml_re(test_sentences, feature_names, f_out):\n",
    "    for test_sentence in test_sentences:\n",
    "        X_test_dict, y_test = extract_features_sent_re(test_sentence, w_size, feature_names)\n",
    "        # Vectorize the test sentence and one hot encoding\n",
    "        X_test = vec.transform(X_test_dict)\n",
    "        # Predicts the chunks and returns numbers\n",
    "        y_test_predicted = classifier.predict(X_test)\n",
    "        # Appends the predicted chunks as a last column and saves the rows\n",
    "        rows = test_sentence.splitlines()\n",
    "        rows = [rows[i] + ' ' + y_test_predicted[i] for i in range(len(rows))]\n",
    "        for row in rows:\n",
    "            f_out.write(row + '\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the features...\n",
      "Encoding the features...\n",
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Predicting the chunks in the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       0.71      0.51      0.59       438\n",
      "     B-ADVP       0.75      0.76      0.75       866\n",
      "    B-CONJP       0.00      0.00      0.00         9\n",
      "     B-INTJ       1.00      0.50      0.67         2\n",
      "      B-LST       0.00      0.00      0.00         5\n",
      "       B-NP       0.94      0.94      0.94     12422\n",
      "       B-PP       0.87      0.96      0.91      4811\n",
      "      B-PRT       0.64      0.17      0.27       106\n",
      "     B-SBAR       0.81      0.30      0.43       535\n",
      "       B-VP       0.93      0.94      0.93      4658\n",
      "     I-ADJP       0.69      0.45      0.55       167\n",
      "     I-ADVP       0.56      0.37      0.45        89\n",
      "    I-CONJP       0.40      0.15      0.22        13\n",
      "      I-LST       0.00      0.00      0.00         2\n",
      "       I-NP       0.95      0.93      0.94     14376\n",
      "       I-PP       1.00      0.02      0.04        48\n",
      "     I-SBAR       0.00      0.00      0.00         4\n",
      "       I-VP       0.91      0.94      0.93      2646\n",
      "          O       0.94      0.96      0.95      6180\n",
      "\n",
      "avg / total       0.92      0.92      0.92     47377\n",
      "\n",
      "\n",
      "Predicting the test set...\n",
      "perl conlleval.txt <outml_re\n",
      "processed 47377 tokens with 23852 phrases; found: 24428 phrases; correct: 20995.\n",
      "accuracy:  92.33%; precision:  85.95%; recall:  88.02%; FB1:  86.97\n",
      "             ADJP: precision:  59.50%; recall:  49.32%; FB1:  53.93  363\n",
      "             ADVP: precision:  71.48%; recall:  73.21%; FB1:  72.33  887\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  5\n",
      "             INTJ: precision: 100.00%; recall:  50.00%; FB1:  66.67  1\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  86.66%; recall:  89.14%; FB1:  87.88  12778\n",
      "               PP: precision:  86.49%; recall:  96.05%; FB1:  91.02  5343\n",
      "              PRT: precision:  64.29%; recall:  16.98%; FB1:  26.87  28\n",
      "             SBAR: precision:  81.44%; recall:  29.53%; FB1:  43.35  194\n",
      "               VP: precision:  88.51%; recall:  91.76%; FB1:  90.10  4829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']\n",
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "   \n",
    "print(\"Extracting the features...\")\n",
    "\n",
    "X_dict, y = extract_features_re(train_sentences, w_size, feature_names)\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)\n",
    "# The statement below will swallow a considerable memory\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#print(vec.get_feature_names())\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model = classifier.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "test_corpus = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/test.txt\").read()\n",
    "test_sentences = str(test_corpus,'utf-8').split('\\n\\n')\n",
    "test_sentences.remove(test_sentences[-1]) # Last element needs to be removed\n",
    "\n",
    "print(\"Predicting the chunks in the test set...\")\n",
    "X_test_dict, y_test = extract_features_re(test_sentences, w_size, feature_names)\n",
    "\n",
    "# Vectorize the test set and one-hot encoding\n",
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_test_predicted)))\n",
    "\n",
    "\n",
    "print(\"Predicting the test set...\")\n",
    "f_out = open('outml_re', 'w')\n",
    "predict_ml_re(test_sentences, feature_names, f_out)\n",
    "\n",
    "cmd_2 = \"perl conlleval.txt <outml_re\"\n",
    "print(cmd_2)\n",
    "\n",
    "p = subprocess.Popen(cmd_2, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the classifier used in the program? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the features...\n",
      "Encoding the features...\n"
     ]
    }
   ],
   "source": [
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2','pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2']\n",
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "   \n",
    "print(\"Extracting the features...\")\n",
    "\n",
    "X_dict, y = extract_features(train_sentences, w_size, feature_names)\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)\n",
    "# The statement below will swallow a considerable memory\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "Predicting the chunks in the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       0.74      0.69      0.72       438\n",
      "     B-ADVP       0.81      0.78      0.80       866\n",
      "    B-CONJP       0.50      0.56      0.53         9\n",
      "     B-INTJ       1.00      0.50      0.67         2\n",
      "      B-LST       0.00      0.00      0.00         5\n",
      "       B-NP       0.95      0.96      0.96     12422\n",
      "       B-PP       0.96      0.97      0.97      4811\n",
      "      B-PRT       0.72      0.76      0.74       106\n",
      "     B-SBAR       0.86      0.81      0.83       535\n",
      "       B-VP       0.95      0.95      0.95      4658\n",
      "     I-ADJP       0.74      0.61      0.67       167\n",
      "     I-ADVP       0.59      0.56      0.57        89\n",
      "    I-CONJP       0.53      0.62      0.57        13\n",
      "      I-LST       0.00      0.00      0.00         2\n",
      "       I-NP       0.96      0.96      0.96     14376\n",
      "       I-PP       0.74      0.71      0.72        48\n",
      "     I-SBAR       0.20      0.75      0.32         4\n",
      "       I-VP       0.94      0.94      0.94      2646\n",
      "          O       0.96      0.96      0.96      6180\n",
      "\n",
      "avg / total       0.95      0.95      0.95     47377\n",
      "\n",
      "\n",
      "Predicting the test set...\n",
      "perl conlleval.txt <outml_decision_tree\n",
      "processed 47377 tokens with 23852 phrases; found: 24349 phrases; correct: 21938.\n",
      "accuracy:  94.76%; precision:  90.10%; recall:  91.98%; FB1:  91.03\n",
      "             ADJP: precision:  66.52%; recall:  68.04%; FB1:  67.27  448\n",
      "             ADVP: precision:  77.80%; recall:  76.91%; FB1:  77.35  856\n",
      "            CONJP: precision:  30.77%; recall:  44.44%; FB1:  36.36  13\n",
      "             INTJ: precision: 100.00%; recall:  50.00%; FB1:  66.67  1\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  89.56%; recall:  92.26%; FB1:  90.89  12796\n",
      "               PP: precision:  96.32%; recall:  97.32%; FB1:  96.82  4861\n",
      "              PRT: precision:  71.68%; recall:  76.42%; FB1:  73.97  113\n",
      "             SBAR: precision:  85.80%; recall:  81.31%; FB1:  83.49  507\n",
      "               VP: precision:  90.68%; recall:  92.55%; FB1:  91.61  4754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "model = classifier.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "print(\"Predicting the chunks in the test set...\")\n",
    "X_test_dict, y_test = extract_features(test_sentences, w_size, feature_names)\n",
    "\n",
    "# Vectorize the test set and one-hot encoding\n",
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_test_predicted)))\n",
    "\n",
    "print(\"Predicting the test set...\")\n",
    "f_out = open('outml_decision_tree', 'w')\n",
    "predict_ml(test_sentences, feature_names, f_out)\n",
    "\n",
    "cmd_2 = \"perl conlleval.txt <outml_decision_tree\"\n",
    "print(cmd_2)\n",
    "\n",
    "p = subprocess.Popen(cmd_2, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "Predicting the chunks in the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=5, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       0.79      0.62      0.69       438\n",
      "     B-ADVP       0.81      0.76      0.78       866\n",
      "    B-CONJP       0.47      0.78      0.58         9\n",
      "     B-INTJ       0.07      1.00      0.13         2\n",
      "      B-LST       0.00      0.00      0.00         5\n",
      "       B-NP       0.95      0.94      0.95     12422\n",
      "       B-PP       0.97      0.96      0.96      4811\n",
      "      B-PRT       0.72      0.66      0.69       106\n",
      "     B-SBAR       0.85      0.82      0.84       535\n",
      "      B-UCP       0.00      0.00      0.00         0\n",
      "       B-VP       0.94      0.94      0.94      4658\n",
      "     I-ADJP       0.70      0.50      0.59       167\n",
      "     I-ADVP       0.55      0.47      0.51        89\n",
      "    I-CONJP       0.59      0.77      0.67        13\n",
      "     I-INTJ       0.00      0.00      0.00         0\n",
      "      I-LST       0.00      0.00      0.00         2\n",
      "       I-NP       0.95      0.94      0.95     14376\n",
      "       I-PP       0.78      0.58      0.67        48\n",
      "      I-PRT       0.00      0.00      0.00         0\n",
      "     I-SBAR       0.11      0.75      0.19         4\n",
      "      I-UCP       0.00      0.00      0.00         0\n",
      "       I-VP       0.90      0.94      0.92      2646\n",
      "          O       0.95      0.96      0.95      6180\n",
      "\n",
      "avg / total       0.94      0.94      0.94     47377\n",
      "\n",
      "\n",
      "Predicting the test set...\n",
      "perl conlleval.txt <outml_Perceptron\n",
      "processed 47377 tokens with 23852 phrases; found: 24562 phrases; correct: 21508.\n",
      "accuracy:  93.62%; precision:  87.57%; recall:  90.17%; FB1:  88.85\n",
      "             ADJP: precision:  65.09%; recall:  59.59%; FB1:  62.22  401\n",
      "             ADVP: precision:  75.98%; recall:  74.13%; FB1:  75.04  845\n",
      "            CONJP: precision:  31.58%; recall:  66.67%; FB1:  42.86  19\n",
      "             INTJ: precision:   5.71%; recall: 100.00%; FB1:  10.81  35\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  6\n",
      "               NP: precision:  87.27%; recall:  90.16%; FB1:  88.69  12834\n",
      "               PP: precision:  96.30%; recall:  96.34%; FB1:  96.32  4813\n",
      "              PRT: precision:  47.26%; recall:  65.09%; FB1:  54.76  146\n",
      "             SBAR: precision:  84.47%; recall:  83.36%; FB1:  83.91  528\n",
      "              UCP: precision:   0.00%; recall:   0.00%; FB1:   0.00  127\n",
      "               VP: precision:  88.33%; recall:  91.18%; FB1:  89.73  4808\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = Perceptron()\n",
    "model = classifier.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "print(\"Predicting the chunks in the test set...\")\n",
    "X_test_dict, y_test = extract_features(test_sentences, w_size, feature_names)\n",
    "\n",
    "# Vectorize the test set and one-hot encoding\n",
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_test_predicted)))\n",
    "\n",
    "print(\"Predicting the test set...\")\n",
    "f_out = open('outml_Perceptron', 'w')\n",
    "predict_ml(test_sentences, feature_names, f_out)\n",
    "\n",
    "cmd_2 = \"perl conlleval.txt <outml_Perceptron\"\n",
    "print(cmd_2)\n",
    "\n",
    "p = subprocess.Popen(cmd_2, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Chunker\n",
    "#### Implement one of these two options, the first one being easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complement the feature vector used in the previous section with the two dynamic features, c i-2 , c i-1 , and train a new model. You will need to modify the extract_features_sent and predict functions.\n",
    "In his experiments, your teacher obtained a F1 score of 92.65 with logistic regression and a lbfgs solver and automatic multiclass;\n",
    "\n",
    "#### You need to reach a global F1 score of 92 to pass this laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_sent_im(sentence, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Extract the features from one sentence\n",
    "    returns X and y, where X is a list of dictionaries and\n",
    "    y is a list of symbols\n",
    "    :param sentence:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # We pad the sentence to extract the context window more easily\n",
    "    start = \"BOS BOS BOS\\n\"\n",
    "    end = \"\\nEOS EOS EOS\"\n",
    "    start *= w_size\n",
    "    end *= w_size\n",
    "    sentence = start + sentence\n",
    "    sentence += end\n",
    "\n",
    "    # Each sentence is a list of rows\n",
    "    sentence = sentence.splitlines()\n",
    "    padded_sentence = list()\n",
    "    for line in sentence:\n",
    "        line = line.split()\n",
    "        padded_sentence.append(line)\n",
    "    # print(padded_sentence)\n",
    "\n",
    "    # We extract the features and the classes\n",
    "    # X contains is a list of features, where each feature vector is a dictionary\n",
    "    # y is the list of classes\n",
    "    X = list()\n",
    "    y = list()\n",
    "    for i in range(len(padded_sentence) - 2 * w_size):\n",
    "        # x is a row of X\n",
    "        x = list()\n",
    "        # The words in lower case\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][0].lower())\n",
    "        # The POS\n",
    "        for j in range(2 * w_size + 1):\n",
    "            x.append(padded_sentence[i + j][1])\n",
    "        # The chunks (Up to the word)\n",
    "        for j in range(w_size):\n",
    "            x.append(padded_sentence[i + j][2])\n",
    "        # We represent the feature vector as a dictionary\n",
    "        X.append(dict(zip(feature_names, x)))\n",
    "        # The classes are stored in a list\n",
    "        y.append(padded_sentence[i + w_size][2])\n",
    "    return X, y\n",
    "\n",
    "def extract_features_im(sentences, w_size, feature_names):\n",
    "    \"\"\"\n",
    "    Builds X matrix and y vector\n",
    "    X is a list of dictionaries and y is a list\n",
    "    :param sentences:\n",
    "    :param w_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X_l = []\n",
    "    y_l = []\n",
    "    for sentence in sentences:\n",
    "        X, y = extract_features_sent_im(sentence, w_size, feature_names)\n",
    "        X_l.extend(X)\n",
    "        y_l.extend(y)\n",
    "    return X_l, y_l\n",
    "\n",
    "def predict_ml_im(test_sentences, feature_names, f_out):\n",
    "    for test_sentence in test_sentences:\n",
    "        X_test_dict, y_test = extract_features_sent(test_sentence, w_size, feature_names)\n",
    "        # Vectorize the test sentence and one hot encoding\n",
    "        X_test = vec.transform(X_test_dict)\n",
    "        # Predicts the chunks and returns numbers\n",
    "        y_test_predicted = classifier.predict(X_test)\n",
    "        # Appends the predicted chunks as a last column and saves the rows\n",
    "        rows = test_sentence.splitlines()\n",
    "        rows = [rows[i] + ' ' + y_test_predicted[i] for i in range(len(rows))]\n",
    "        for row in rows:\n",
    "            f_out.write(row + '\\n')\n",
    "        f_out.write('\\n')\n",
    "    f_out.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the features...\n",
      "Encoding the features...\n",
      "Training the model...\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Predicting the chunks in the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier LogisticRegression(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     B-ADJP       0.47      0.72      0.57       438\n",
      "     B-ADVP       0.71      0.81      0.76       866\n",
      "    B-CONJP       0.50      0.67      0.57         9\n",
      "     B-INTJ       1.00      0.50      0.67         2\n",
      "      B-LST       0.00      0.00      0.00         5\n",
      "       B-NP       0.57      0.98      0.72     12422\n",
      "       B-PP       0.94      0.98      0.96      4811\n",
      "      B-PRT       0.78      0.57      0.66       106\n",
      "     B-SBAR       0.85      0.85      0.85       535\n",
      "       B-VP       0.72      0.96      0.82      4658\n",
      "     I-ADJP       0.33      0.08      0.13       167\n",
      "     I-ADVP       0.30      0.12      0.17        89\n",
      "    I-CONJP       1.00      0.69      0.82        13\n",
      "      I-LST       0.00      0.00      0.00         2\n",
      "       I-NP       0.99      0.31      0.47     14376\n",
      "       I-PP       0.92      0.25      0.39        48\n",
      "     I-SBAR       0.00      0.00      0.00         4\n",
      "       I-VP       0.90      0.35      0.51      2646\n",
      "          O       0.87      0.98      0.92      6180\n",
      "\n",
      "avg / total       0.81      0.73      0.69     47377\n",
      "\n",
      "\n",
      "Predicting the test set...\n",
      "perl conlleval.txt <outml_improved\n",
      "processed 47377 tokens with 23852 phrases; found: 35142 phrases; correct: 15405.\n",
      "accuracy:  72.51%; precision:  43.84%; recall:  64.59%; FB1:  52.23\n",
      "             ADJP: precision:  35.81%; recall:  56.16%; FB1:  43.73  687\n",
      "             ADVP: precision:  65.88%; recall:  76.91%; FB1:  70.96  1011\n",
      "            CONJP: precision:  38.46%; recall:  55.56%; FB1:  45.45  13\n",
      "             INTJ: precision: 100.00%; recall:  50.00%; FB1:  66.67  1\n",
      "              LST: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  28.72%; recall:  49.28%; FB1:  36.29  21317\n",
      "               PP: precision:  93.50%; recall:  97.80%; FB1:  95.60  5032\n",
      "              PRT: precision:  77.92%; recall:  56.60%; FB1:  65.57  77\n",
      "             SBAR: precision:  84.77%; recall:  84.30%; FB1:  84.54  532\n",
      "               VP: precision:  48.66%; recall:  67.60%; FB1:  56.59  6472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_names = ['word_n2', 'word_n1', 'word', 'word_p1', 'word_p2',\n",
    "                 'pos_n2', 'pos_n1', 'pos', 'pos_p1', 'pos_p2',\n",
    "                 'c_n2', 'c_n1']\n",
    "w_size = 2  # The size of the context window to the left and right of the word\n",
    "\n",
    "b_train_text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/conll2000/train.txt\").read() # Open file and read\n",
    "train_text = str(b_train_text,'utf-8')\n",
    "train_text = train_text.strip()\n",
    "train_sentences = train_text.split('\\n\\n')\n",
    "\n",
    "print(\"Extracting the features...\")\n",
    "\n",
    "X_dict, y = extract_features_im(train_sentences, w_size, feature_names)\n",
    "\n",
    "print(\"Encoding the features...\")\n",
    "# Vectorize the feature matrix and carry out a one-hot encoding\n",
    "vec = DictVectorizer(sparse=True)\n",
    "X = vec.fit_transform(X_dict)\n",
    "# The statement below will swallow a considerable memory\n",
    "#X = vec.fit_transform(X_dict).toarray()\n",
    "#print(vec.get_feature_names())\n",
    "\n",
    "print(\"Training the model...\")\n",
    "classifier = linear_model.LogisticRegression(penalty='l2', dual=True, solver='liblinear')\n",
    "model = classifier.fit(X, y)\n",
    "print(model)\n",
    "\n",
    "print(\"Predicting the chunks in the test set...\")\n",
    "X_test_dict, y_test = extract_features(test_sentences, w_size, feature_names)\n",
    "\n",
    "# Vectorize the test set and one-hot encoding\n",
    "X_test = vec.transform(X_test_dict)  # Possible to add: .toarray()\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, y_test_predicted)))\n",
    "\n",
    "\n",
    "print(\"Predicting the test set...\")\n",
    "f_out = open('outml_improved', 'w')\n",
    "predict_ml_im(test_sentences, feature_names, f_out)\n",
    "\n",
    "cmd_2 = \"perl conlleval.txt <outml_improved\"\n",
    "print(cmd_2)\n",
    "\n",
    "p = subprocess.Popen(cmd_2, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you know what beam search is, apply it using the probability output of logistic regression or the score if you use support vector machines.\n",
    "With the same classifier and a beam diameter of 5, your teacher obtained 92.87."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will read the article, Contextual String Embeddings for Sequence Labeling by Akbik et al. (2018) and you will outline the main differences between their system and yours. A LSTM is a type of recurrent neural network, while CRF is a sort of beam search. https://www.aclweb.org/anthology/C18-1139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off they used a recurrent neural network instead of logistic regression. Also they already in abstract they mension that \"Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use.\". We train our model with explicit notion of words / sequence of words not charaters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will tell the performance they reach on the corpus you used in this laboratory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They scored 96.72±0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
