{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Building dictionary with Selma Lagerlöf novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laboration 1 in EDAN20 @ LTH - http://cs.lth.se/edan20/coursework/assignment-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jonatan Kronander - elt15jkr@student.lu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The objectives of this assignment are to:\n",
    "\n",
    "-Write a program that collects all the words from a set of documents\n",
    "\n",
    "-Build an index from the words\n",
    "\n",
    "-Know what indexing is\n",
    "\n",
    "-Represent a document using the Tf.Idf value\n",
    "\n",
    "-Write a short report of 1 to 2 pages on the assignment\n",
    "\n",
    "-Read a short text on an industrial system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing one file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The index file will contain all the unique words in the document, where each word is associated with the list of its positions in the document.\n",
    "\n",
    "* You will represent this index as a dictionary where the keys will be the words and the values, the lists of positions\n",
    "\n",
    "* As words, you will consider all the strings of letters that you will set in lower case. You will not index the rest (i.e. numbers or symbols).\n",
    "\n",
    "* To extract the words, you will use Unicode regular expressions. Do not use \\w+, for instance, but the Unicode equivalent. The word positions will correspond to the number of characters from the beginning of the file. (The word offset from the beginning)\n",
    "\n",
    "* You will use finditer() to find the positions of the words. This will return you match objects, where you will get the matches and the positions with the group() and start() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(\"Selma/bannlyst.txt\").read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def toLowercase(matchobj):\n",
    "    \"\"\"\n",
    "    Helper function\n",
    "    \"\"\"\n",
    "    return matchobj.group(1).lower()\n",
    "\n",
    "def textLowerList(text):\n",
    "    \"\"\"\n",
    "    Make text lowercase and put into list\n",
    "    \n",
    "    :param string:\n",
    "    :return list:\n",
    "    \"\"\"\n",
    "    \n",
    "    textLow = re.sub(r'([A-ZÅÄÖ])', toLowercase, text) # Lowercase all characters\n",
    "    \n",
    "    stringList = re.findall(r\"[a-zåäö]+\",textLow) # This finds all words from a txt file. r\"[a-zåäö]+ equal to r\"\\w+\" \n",
    "    \n",
    "    return stringList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowercase(matchobj):\n",
    "    return matchobj.group(1).lower()\n",
    "\n",
    "def string2dict(text):\n",
    "    \"\"\"\n",
    "    Creates a dict with (word:list[index apperences]) from input string\n",
    "    \n",
    "    Input string, string\n",
    "    Output dict\n",
    "    \"\"\"\n",
    "    textLow = re.sub(r'([A-ZÅÄÖ])', toLowercase, text) # Lowercase all characters\n",
    "    \n",
    "    stringList = textLowerList(text)\n",
    "    \n",
    "    stringDict = {key : list([]) for key in stringList}\n",
    "    \n",
    "    for m in re.finditer(r\"[a-zåäö]+\", textLow): # Iterate thorugh every word\n",
    "        s = m.start()\n",
    "        e = m.end()\n",
    "        \n",
    "        word = textLow[s:e]\n",
    "        stringDict[word].append(s)\n",
    "\n",
    "    \n",
    "    return stringDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txtDict = string2dict(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7924"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txtDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with bannlyst text\n",
    "The word gjord occurs three times in the text at positions 8551, 183692, and 220875, uppklarnande, once at position 8567, and stjärnor, once at position 8590. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8551, 183692, 220875]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDict['gjord']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8567]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDict['uppklarnande']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8590]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtDict['stjärnor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You will use the pickle package to write your dictionary in an file, see https://wiki.python.org/moin/UsingPickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open('BannlystTxtDict.pickle', 'wb') as handle:\n",
    "#    pickle.dump(txtDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BannlystTxtDict.pickle', 'rb') as handle:\n",
    "    BannlystTxtDict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BannlystTxtDict == txtDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the content of a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that reads all the files in a folder with a specific suffix (txt). You will need the Python os package, see https://docs.python.org/3/library/os.html. You will return the file names in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_files(fileDir, suffix):\n",
    "    \"\"\"\n",
    "    Returns all the files in a folder ending with suffix\n",
    "    :param filedir:\n",
    "    :param suffix:\n",
    "    :return: the list of file names\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for file in os.listdir(fileDir):\n",
    "        if file.endswith(suffix):\n",
    "            files.append(file)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(\"selma\", \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['troll.txt',\n",
       " 'kejsaren.txt',\n",
       " 'marbacka.txt',\n",
       " 'herrgard.txt',\n",
       " 'nils.txt',\n",
       " 'osynliga.txt',\n",
       " 'jerusalem.txt',\n",
       " 'bannlyst.txt',\n",
       " 'gosta.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a master index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete your program with the creation of master index, where you will associate each word of the corpus with the files, where it occur and its positions. (a posting list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowercase(matchobj):\n",
    "    return matchobj.group(1).lower()\n",
    "    \n",
    "def addAll(fileDir,files): \n",
    "    \"\"\"\n",
    "    This function takes way to long. Do not iterate word in dict but build dict directly. \n",
    "    \n",
    "    Reads all files in list and matches to txt files\n",
    "    :param dir:\n",
    "    :param files:\n",
    "    :return dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    totText = []\n",
    "    \n",
    "    for file in files:\n",
    "        text = open(fileDir+\"/\"+file).read() \n",
    "        stringList = textLowerList(text)\n",
    "        totText.extend(stringList)\n",
    "        \n",
    "    masterDict = {word : {file : list([]) for file in files} for word in totText}\n",
    "    \n",
    "    for file in files:\n",
    "        text = open(fileDir+\"/\"+file).read()\n",
    "        txtDict = string2dict(text)         \n",
    "        for word in txtDict.keys():\n",
    "            masterDict[word][file] = txtDict[word]\n",
    "\n",
    "    return masterDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "masterDict = addAll('selma',files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('masterDict.pickle', 'wb') as handle:\n",
    "#    pickle.dump(masterDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('masterDict.pickle', 'rb') as handle:\n",
    "    masterDict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of master dict. Below is an except of the master index with the words samlar and ände:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'samlar': {'nils.txt': [53499, 120336], 'gosta.txt': [317119, 414300, 543686],'osynliga.txt': [410995, 871322]},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': [],\n",
       " 'kejsaren.txt': [],\n",
       " 'marbacka.txt': [],\n",
       " 'herrgard.txt': [],\n",
       " 'nils.txt': [53499, 120336],\n",
       " 'osynliga.txt': [410995, 871322],\n",
       " 'jerusalem.txt': [],\n",
       " 'bannlyst.txt': [],\n",
       " 'gosta.txt': [317119, 414300, 543686]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterDict[\"samlar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'ände':{'nils.txt': [3991],'kejsaren.txt': [51100],'marbacka.txt': [374231],'troll.txt': [39726],'osynliga.txt': [742747]},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': [39726],\n",
       " 'kejsaren.txt': [51100],\n",
       " 'marbacka.txt': [374231],\n",
       " 'herrgard.txt': [],\n",
       " 'nils.txt': [3991],\n",
       " 'osynliga.txt': [742747],\n",
       " 'jerusalem.txt': [],\n",
       " 'bannlyst.txt': [],\n",
       " 'gosta.txt': []}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterDict[\"ände\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Documents with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have created the index, you will represent each document in your corpus as a word vector. You will define the value of a word in a document with the tf-idf metric. Tf will be the relative frequency of the term in the document and idf, the logarithm base 10 of the inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tiIdf(masterDict):\n",
    "    \"\"\"\n",
    "    This function takes way to long. Do not iterate word in dict but build dict directly. \n",
    "    \n",
    "    Creates a ft-idf dict from all files.\n",
    "    https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/\n",
    "    :Param dict:\n",
    "    :return dict:\n",
    "    \"\"\"\n",
    "    \n",
    "    tfIdfDict = masterDict.copy()\n",
    "    j = 0\n",
    "    \n",
    "    lenText = {}\n",
    "    for file in masterDict['nils']: # Read total nbr of words in each text\n",
    "        text = open('selma'+\"/\"+file).read()\n",
    "        lenText[file] = len(textLowerList(text)) # nbr of words in textfile\n",
    "        \n",
    "    for word in masterDict: \n",
    "        #idf will be the logarithm base 10 of the inverse document frequency.\n",
    "        nbrKeys = len(masterDict[word].keys())\n",
    "        dictValues = masterDict[word].values()\n",
    "        lenDictValues = len(dictValues)\n",
    "        \n",
    "        i = 0 \n",
    "        for fileList in dictValues: # Count nbr of empty list. (There is probably a better way to do this)\n",
    "            if not fileList:\n",
    "                i = i + 1\n",
    "\n",
    "        df = (lenDictValues-i)\n",
    "        \n",
    "        idf = math.log10(nbrKeys/df)\n",
    "        \n",
    "        for file in masterDict[word]:\n",
    "            # Tf will be the relative frequency of the term in the document \n",
    "            lenWordVec = len(masterDict[word][file]) # nbr of occurencies of word\n",
    "\n",
    "            tf =  lenWordVec / lenText[file]   \n",
    "            tfIdfDict[word][file] = tf*idf\n",
    "        \n",
    "    return tfIdfDict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfIdfDict = tiIdf(masterDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('tfIdfDict.pickle', 'wb') as handle:\n",
    "#    pickle.dump(tfIdfDict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfIdfDict.pickle', 'rb') as handle:\n",
    "    tfIdfDict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test of tf idf:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "känna :: bannlyst.txt 0.0, gosta.txt 0.0, herrgard.txt 0.0, jerusalem.txt 0.0, nils.txt 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': 0.0,\n",
       " 'kejsaren.txt': 0.0,\n",
       " 'marbacka.txt': 0.0,\n",
       " 'herrgard.txt': 0.0,\n",
       " 'nils.txt': 0.0,\n",
       " 'osynliga.txt': 0.0,\n",
       " 'jerusalem.txt': 0.0,\n",
       " 'bannlyst.txt': 0.0,\n",
       " 'gosta.txt': 0.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfDict['känna']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gås :: bannlyst.txt 0.0, gosta.txt 0.0, herrgard.txt 0.0, jerusalem.txt 0.0, nils.txt 0.00010123719421964931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': 0.0,\n",
       " 'kejsaren.txt': 0.0,\n",
       " 'marbacka.txt': 0.0,\n",
       " 'herrgard.txt': 0.0,\n",
       " 'nils.txt': 0.00010122798897871197,\n",
       " 'osynliga.txt': 0.0,\n",
       " 'jerusalem.txt': 0.0,\n",
       " 'bannlyst.txt': 0.0,\n",
       " 'gosta.txt': 0.0}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfDict['gås']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nils :: bannlyst.txt 0.0, gosta.txt 0.0, herrgard.txt 0.0 jerusalem.txt 4.778415355159037e-06, nils.txt 9.801209641132888e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': 3.6573040073255117e-06,\n",
       " 'kejsaren.txt': 8.084895961972068e-06,\n",
       " 'marbacka.txt': 7.587798323647353e-06,\n",
       " 'herrgard.txt': 0.0,\n",
       " 'nils.txt': 9.800318442034381e-05,\n",
       " 'osynliga.txt': 0.0,\n",
       " 'jerusalem.txt': 4.7779383543374685e-06,\n",
       " 'bannlyst.txt': 0.0,\n",
       " 'gosta.txt': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfDict['nils']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "et :: bannlyst.txt 6.2846093167673765e-06, gosta.txt 0.0, herrgard.txt 0.0, jerusalem.txt 0.0, nils.txt 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'troll.txt': 0.0,\n",
       " 'kejsaren.txt': 6.044482862097453e-05,\n",
       " 'marbacka.txt': 1.4182098676366573e-05,\n",
       " 'herrgard.txt': 0.0,\n",
       " 'nils.txt': 0.0,\n",
       " 'osynliga.txt': 0.0,\n",
       " 'jerusalem.txt': 0.0,\n",
       " 'bannlyst.txt': 6.284526537403352e-06,\n",
       " 'gosta.txt': 0.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfDict['et']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the cosine similarity, compare all the pairs of documents with their tfidf representation and present your results in a matrix. You will include this matrix in your report.\n",
    "\n",
    "Give the name of the two novels that are the most similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are the document representations in term of words. Rows: documents, Col: words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "docMatrix = np.zeros((9,len(tfIdfDict.keys())))\n",
    "wordList = tfIdfDict.keys()\n",
    "fileList = tfIdfDict['nils']\n",
    "\n",
    "for i, word in enumerate(wordList):\n",
    "    for j, file in enumerate(fileList):\n",
    "        docMatrix[j,i] = tfIdfDict[word][file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Position: ( 0 , 1 ) has maximum val\n",
      "That correspond to text: troll.txt and kejsaren.txt Hence they are most similar with cosine value: 0.0883242211534\n"
     ]
    }
   ],
   "source": [
    "import numpy as np; import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df = pd.DataFrame(docMatrix)\n",
    "\n",
    "simularityMatrix = cosine_similarity(df)\n",
    "\n",
    "print()\n",
    "\n",
    "index = simularityMatrix[:,:].flatten().argsort()[-10:][::-1][9] # This is the \n",
    "\n",
    "print(\"Position: (\", index//9, \",\", index, \") has maximum val\")\n",
    "\n",
    "print(\"That correspond to text:\", list(fileList.keys())[index//9], \"and\", list(fileList.keys())[index], \"Hence they are most similar with cosine value:\", simularityMatrix[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = troll.txt\n",
      "1 = kejsaren.txt\n",
      "2 = marbacka.txt\n",
      "3 = herrgard.txt\n",
      "4 = nils.txt\n",
      "5 = osynliga.txt\n",
      "6 = jerusalem.txt\n",
      "7 = bannlyst.txt\n",
      "8 = gosta.txt\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(np.log(simularityMatrix))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "for i,file in enumerate(list(fileList.keys())):\n",
    "    print(i, \"=\", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1 TEXT 2 Simularity Value\n",
      "troll.txt troll.txt 1.0\n",
      "troll.txt kejsaren.txt 0.0883242211534\n",
      "troll.txt marbacka.txt 0.0236866510836\n",
      "troll.txt herrgard.txt 0.00398547315992\n",
      "troll.txt nils.txt 0.0193837084392\n",
      "troll.txt osynliga.txt 0.0282769699387\n",
      "troll.txt jerusalem.txt 0.00707951128926\n",
      "troll.txt bannlyst.txt 0.0073414645024\n",
      "troll.txt gosta.txt 0.0324358645237\n",
      "kejsaren.txt troll.txt 0.0883242211534\n",
      "kejsaren.txt kejsaren.txt 1.0\n",
      "kejsaren.txt marbacka.txt 0.0445954870002\n",
      "kejsaren.txt herrgard.txt 0.000891615732519\n",
      "kejsaren.txt nils.txt 0.00437266259504\n",
      "kejsaren.txt osynliga.txt 0.00552744271082\n",
      "kejsaren.txt jerusalem.txt 0.0020577102754\n",
      "kejsaren.txt bannlyst.txt 0.00214438176036\n",
      "kejsaren.txt gosta.txt 0.00649576796948\n",
      "marbacka.txt troll.txt 0.0236866510836\n",
      "marbacka.txt kejsaren.txt 0.0445954870002\n",
      "marbacka.txt marbacka.txt 1.0\n",
      "marbacka.txt herrgard.txt 0.0117053763102\n",
      "marbacka.txt nils.txt 0.0402253390059\n",
      "marbacka.txt osynliga.txt 0.0421493424997\n",
      "marbacka.txt jerusalem.txt 0.0129114844634\n",
      "marbacka.txt bannlyst.txt 0.00509843725241\n",
      "marbacka.txt gosta.txt 0.0285374243554\n",
      "herrgard.txt troll.txt 0.00398547315992\n",
      "herrgard.txt kejsaren.txt 0.000891615732519\n",
      "herrgard.txt marbacka.txt 0.0117053763102\n",
      "herrgard.txt herrgard.txt 1.0\n",
      "herrgard.txt nils.txt 0.0141469810598\n",
      "herrgard.txt osynliga.txt 0.0212600615683\n",
      "herrgard.txt jerusalem.txt 0.00762103417282\n",
      "herrgard.txt bannlyst.txt 0.00129803415752\n",
      "herrgard.txt gosta.txt 0.015107711443\n",
      "nils.txt troll.txt 0.0193837084392\n",
      "nils.txt kejsaren.txt 0.00437266259504\n",
      "nils.txt marbacka.txt 0.0402253390059\n",
      "nils.txt herrgard.txt 0.0141469810598\n",
      "nils.txt nils.txt 1.0\n",
      "nils.txt osynliga.txt 0.0321302745291\n",
      "nils.txt jerusalem.txt 0.0130908544942\n",
      "nils.txt bannlyst.txt 0.00534538567103\n",
      "nils.txt gosta.txt 0.0249484513535\n",
      "osynliga.txt troll.txt 0.0282769699387\n",
      "osynliga.txt kejsaren.txt 0.00552744271082\n",
      "osynliga.txt marbacka.txt 0.0421493424997\n",
      "osynliga.txt herrgard.txt 0.0212600615683\n",
      "osynliga.txt nils.txt 0.0321302745291\n",
      "osynliga.txt osynliga.txt 1.0\n",
      "osynliga.txt jerusalem.txt 0.0418537861278\n",
      "osynliga.txt bannlyst.txt 0.00599202648917\n",
      "osynliga.txt gosta.txt 0.0556143735945\n",
      "jerusalem.txt troll.txt 0.00707951128926\n",
      "jerusalem.txt kejsaren.txt 0.0020577102754\n",
      "jerusalem.txt marbacka.txt 0.0129114844634\n",
      "jerusalem.txt herrgard.txt 0.00762103417282\n",
      "jerusalem.txt nils.txt 0.0130908544942\n",
      "jerusalem.txt osynliga.txt 0.0418537861278\n",
      "jerusalem.txt jerusalem.txt 1.0\n",
      "jerusalem.txt bannlyst.txt 0.00731895683465\n",
      "jerusalem.txt gosta.txt 0.00851649835039\n",
      "bannlyst.txt troll.txt 0.0073414645024\n",
      "bannlyst.txt kejsaren.txt 0.00214438176036\n",
      "bannlyst.txt marbacka.txt 0.00509843725241\n",
      "bannlyst.txt herrgard.txt 0.00129803415752\n",
      "bannlyst.txt nils.txt 0.00534538567103\n",
      "bannlyst.txt osynliga.txt 0.00599202648917\n",
      "bannlyst.txt jerusalem.txt 0.00731895683465\n",
      "bannlyst.txt bannlyst.txt 1.0\n",
      "bannlyst.txt gosta.txt 0.00522035906598\n",
      "gosta.txt troll.txt 0.0324358645237\n",
      "gosta.txt kejsaren.txt 0.00649576796948\n",
      "gosta.txt marbacka.txt 0.0285374243554\n",
      "gosta.txt herrgard.txt 0.015107711443\n",
      "gosta.txt nils.txt 0.0249484513535\n",
      "gosta.txt osynliga.txt 0.0556143735945\n",
      "gosta.txt jerusalem.txt 0.00851649835039\n",
      "gosta.txt bannlyst.txt 0.00522035906598\n",
      "gosta.txt gosta.txt 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TEXT 1\", \"TEXT 2\", \"Simularity Value\")\n",
    "for i,iFile in enumerate(list(fileList.keys())):\n",
    "    for j, jFile in enumerate(list(fileList.keys())):\n",
    "        print(iFile, jFile, simularityMatrix[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the text: Challenges in Building Large-Scale Information Retrieval Systems about the history of Google indexing by Jeff Dean. In your report, tell how your index encoding is related to what Google did. You must identify the slide where you have the most similar indexing technique and write the slide number in your report. https://static.googleusercontent.com/media/research.google.com/en//people/jeff/WSDM09-keynote.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On slide 14-17 they talk about ways of index partitioning and compare by doing so with doc or by word. Google and we have similar problem where we want to index shards partitioning from a set of documents. Google uses docs as index while we used word, where we had shard subset of words for all docs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
