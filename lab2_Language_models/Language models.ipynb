{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laboration 2 in EDAN20 @ LTH - http://cs.lth.se/edan20/coursework/assignment-2/\n",
    "\n",
    "Author: Jonatan Kronander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Write a program to find n-gram statistics\n",
    "* Compute the probability of a sentence\n",
    "* Know what a language model is\n",
    "* Write a short report of 1 to 2 pages on the assignment\n",
    "* Optionally read a short article on the importance of corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Collect a corpus of at least 750,000 words. Alternatively, you can retrieve a corpus of novels by Selma Lagerlöf from this URL: http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/Selma.txt ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/Selma.txt\").read() # Open file and read\n",
    "strText = str(text,'utf-8') # From bytes to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"Selma.txt\", \"w\")\n",
    "text_file.write(strText)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will check the number of words using the Unix command wc -w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wc -w Selma.txt\n",
      "Number of words in text:  965943 Selma.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd_0 = 'wc -w Selma.txt'\n",
    "print(cmd_0)\n",
    "\n",
    "p = subprocess.Popen(cmd_0, stdout=subprocess.PIPE, shell=True)\n",
    "out_0, err = p.communicate() \n",
    "print(\"Number of words in text:\" + str(out_0,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run the concordance program (https://github.com/pnugues/ilppp/tree/master/programs/ch02/python) to print the lines containing a specific word, for instance Nils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python concord.py Selma.txt Nils 10\n",
      "\n",
      " ----Lines containing Nils is listed below---- \n",
      "\n",
      "Nils Holgersso\n",
      "os Holger Nilssons LV. A\n",
      "tt! Se på Nils gåsapåg! \n",
      "tt! Se på Nils Holgersso\n",
      "ll Holger Nilsson i Väst\n",
      "Jag heter Nils Holgersso\n",
      "det värt, Nils Holgersso\n",
      "att du är Nils gåsapåg, \n",
      "r han var Nils gåsapåg, \n",
      "r och hur Nils gåsapåg h\n",
      "å han var Nils gåsapåg? \n",
      "tiden, då Nils Holgersso\n",
      "honom vad Nils Holgersso\n",
      " året, då Nils Holgersso\n",
      "osta dem. Nils Holgersso\n",
      " sägas om Nils Holgersso\n",
      " där stod Nils Holgersso\n",
      "de syn på Nils Holgersso\n",
      "ässen och Nils Holgersso\n",
      ". Men vad Nils Holgersso\n",
      " du hört, Nils gåsapåg, \n",
      "och om då Nils Holgersso\n",
      "och om nu Nils Holgersso\n",
      "g ut, att Nils Holgersso\n",
      "nte ville Nils Holgersso\n",
      "iden, när Nils Holgersso\n",
      "honom. Om Nils Holgersso\n",
      "gen hand, Nils Holgersso\n",
      "rumle ner Nils Holgersso\n",
      "o, jag är Nils Holgersso\n",
      "tiden, då Nils Holgersso\n",
      "Det står: Nils Holgersso\n",
      "fär innan Nils Holgersso\n",
      "föda, men Nils Holgersso\n",
      " året, då Nils Holgersso\n",
      "ssen över Nils Holgersso\n",
      " dag fick Nils Holgersso\n",
      " året, då Nils Holgersso\n",
      "kten till Nils Holgersso\n",
      "an Holger Nilsson och ha\n",
      "ade, stod Nils Holgersso\n",
      "ugge. Vad Nils Holgersso\n",
      "sådan som Nils Holgersso\n",
      "t mittför Nils Holgersso\n",
      "tiden, då Nils Holgersso\n",
      "klig. Att Nils Holgersso\n",
      "edes hade Nils Holgersso\n",
      "chen. Men Nils Holgersso\n",
      "om dessa. Nils Holgersso\n",
      ". \"Jag är Nils Holgersso\n",
      "e det, då Nils Holgersso\n",
      "de han om Nils Holgersso\n",
      "an Holger Nilsson och ha\n",
      "den lilla Nils Holgersso\n",
      "d is, och Nils Holgersso\n",
      "itt!\" Men Nils Holgersso\n",
      "arna, och Nils Holgersso\n",
      " året, då Nils Holgersso\n",
      " den. Vad Nils Holgersso\n",
      " klorna i Nils Holgersso\n",
      " året, då Nils Holgersso\n",
      " i Holger Nilssons kosta\n",
      "ör Holger Nilssons stuga\n",
      "or Holger Nilssons själv\n",
      "bedrövad, Nils Holgersso\n",
      "på Holger Nilssons torp,\n",
      " kunde ge Nils Holgersso\n",
      "han. 'Men Nils Holgersso\n",
      "t. Holger Nilsson har må\n",
      ". Ja, säg Nils Holgersso\n",
      "Den lille Nils Holgersso\n",
      "Den lille Nils Holgersso\n",
      "Då tyckte Nils Holgersso\n",
      "lja det.\" Nils Holgersso\n",
      "unga. Vad Nils Holgersso\n",
      "os Holger Nilssons Tisda\n",
      "ll Holger Nilssons, och \n",
      "att se på Nils Holgersso\n",
      "olik. Den Nils Holgersso\n",
      "mmen hem, Nils Holgersso\n",
      "tt Holger Nilsson hade v\n",
      "en,\" sade Nils Holgersso\n",
      "ör Holger Nilsson vad so\n",
      "om,\" sade Nils Holgersso\n",
      "din hov?\" Nils Holgersso\n",
      "ar så lik Nils Holgersso\n",
      "t gott om Nils.\" - \"Det \n",
      "r att vår Nils hade gjor\n",
      "tt Holger Nilsson ännu d\n",
      "os Holger Nilssons, när \n",
      "de Holger Nilsson. \"Och \n",
      "de Holger Nilsson. - \"Om\n",
      "närheten. Nils Holgersso\n",
      " lust, du Nils Jansson. \n",
      "Backstugu Nils stå och v\n",
      "Backstugu Nils ska allt \n",
      "ka, Kajsa Nilsdotter, ko\n",
      "var Kajsa Nilsdotter ful\n",
      "t i Kajsa Nilsdotters ör\n",
      "de. Kajsa Nilsdotter had\n",
      "r i Kajsa Nilsdotter, at\n",
      "kte Kajsa Nilsdotter han\n",
      "vad Kajsa Nilsdotter had\n",
      "t i Kajsa Nilsdotters br\n",
      "s i Kajsa Nilsdotters br\n",
      "ksdagsman Nils Andersson\n",
      "en Teodor Nilsson i Vist\n",
      "t är herr Nilsson ifrån \n",
      "ksdagsman Nils Andersson\n",
      " och herr Nilsson i Vist\n",
      "var Börje Nilsson, och h\n",
      "ör Börje. Nilssons brud.\n",
      "ara Börje Nilssons hustr\n",
      "kte Börje Nilssons hustr\n",
      "var Börje Nilsson, och h\n",
      "för Börje Nilssons brud.\n",
      "ara Börje Nilssons hustr\n",
      "kte Börje Nilssons hustr\n",
      " tack för Nils Holgersso\n",
      "detta, du Nils? Du sitte\n",
      "ag om du, Nils, kommer i\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Nils\"\n",
    "width = \" 10\"\n",
    "cmd_1 = \"python concord.py Selma.txt \"+pattern+width\n",
    "print(cmd_1)\n",
    "\n",
    "p = subprocess.Popen(cmd_1, stdout=subprocess.PIPE, shell=True)\n",
    "out_1, err = p.communicate() \n",
    "print(\"\\n ----Lines containing \" + pattern + \" is listed below---- \\n\")\n",
    "print(str(out_1,'utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Run the tokenization program (https://github.com/pnugues/ilppp/tree/master/programs/ch05/python) on your corpus and count the words using the Unix sort and uniq commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLowercase(matchobj):\n",
    "    \"\"\"\n",
    "    Helper function\n",
    "    \"\"\"\n",
    "    return matchobj.group(1).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python tokenizer.py < Selma.txt\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "cmd_2 = \"python tokenizer.py < Selma.txt\"\n",
    "print(cmd_2)\n",
    "\n",
    "p = subprocess.Popen(cmd_2, stdout=subprocess.PIPE, shell=True)\n",
    "out_2, err = p.communicate() \n",
    "\n",
    "stringOut = str(out_2,'utf-8') # Bytes --> String\n",
    "\n",
    "cleanString = re.sub(r'[^A-Za-z ]', '', stringOut) # Remove all non characters (-space)\n",
    "cleanString = re.sub(r'(\\p{Lu})', toLowercase, cleanString) # Lowercase all characters\n",
    "nlString = re.sub(r'[ ]', r'\\n', cleanString) # New line instead of space\n",
    "\n",
    "text_file = open(\"selmaCorpus.txt\", \"w\")\n",
    "text_file.write(nlString)\n",
    "text_file.close()\n",
    "\n",
    "selmaString = open('selmaCorpus.txt', 'r')\n",
    "\n",
    "selmaStringData = selmaString.readlines()\n",
    "selmaStringData = [ x.rstrip() for x in selmaStringData ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sort selmaCorpus.txt | uniq\n"
     ]
    }
   ],
   "source": [
    "# Count words with unix\n",
    "cmd_3 = \"sort selmaCorpus.txt | uniq\"\n",
    "print(cmd_3)\n",
    "\n",
    "p = subprocess.Popen(cmd_3, stdout=subprocess.PIPE, shell=True)\n",
    "out_3, err = p.communicate() \n",
    "\n",
    "stringOut = str(out_3,'utf-8')\n",
    "\n",
    "text_file = open(\"cleanSelmaCorpus.txt\", \"w\")\n",
    "text_file.write(stringOut)\n",
    "text_file.close()\n",
    "\n",
    "selmaString = open('cleanSelmaCorpus.txt', 'r')\n",
    "data = selmaString.readlines()\n",
    "\n",
    "corpusList = [ x.rstrip() for x in data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'a',\n",
       " 'abborrar',\n",
       " 'abborrarna',\n",
       " 'abborre',\n",
       " 'abborren',\n",
       " 'abborrens',\n",
       " 'abborrn',\n",
       " 'abcbok',\n",
       " 'abcdarierna',\n",
       " 'abessinien',\n",
       " 'abessinierna',\n",
       " 'abessiniska',\n",
       " 'abraham',\n",
       " 'abrahams',\n",
       " 'absalon',\n",
       " 'absolut',\n",
       " 'absoluta',\n",
       " 'abstinens',\n",
       " 'abu',\n",
       " 'ach',\n",
       " 'achmed',\n",
       " 'ack',\n",
       " 'ackja',\n",
       " 'ackompanjemanget',\n",
       " 'ackompanjera',\n",
       " 'ackord',\n",
       " 'acquilon',\n",
       " 'acquilons',\n",
       " 'adam',\n",
       " 'ade',\n",
       " 'adeline',\n",
       " 'adelsbrev',\n",
       " 'adelsfrknarna',\n",
       " 'adelsgrd',\n",
       " 'adelskapets',\n",
       " 'adelsman',\n",
       " 'adelsmn',\n",
       " 'aderton',\n",
       " 'adertonde',\n",
       " 'adertonhundrafyra',\n",
       " 'adertonhundratalet',\n",
       " 'adertonhundratjugotalet',\n",
       " 'adertonhundratjugutalet',\n",
       " 'adertonhundratolv',\n",
       " 'adertonhundratre',\n",
       " 'adertonhundratrettiotalet',\n",
       " 'adertonhundratta',\n",
       " 'adertonriga',\n",
       " 'adertonringen']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusList[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 38663 words in the corpus.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(corpusList)) + \" words in the corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Write a program to insert \\< s> and \\< /s> tags to delimit sentences. You can start from the tokenization and modify it. Use a simple heuristics such as: a sentence starts with a capital letter and ends with a period. Estimate roughly the accuracy of your program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python tokenizer.py < Selma.txt\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "\n",
    "cmd_4 = \"python tokenizer.py < Selma.txt\"\n",
    "print(cmd_4)\n",
    "\n",
    "p = subprocess.Popen(cmd_4, stdout=subprocess.PIPE, shell=True)\n",
    "out_4, err = p.communicate() \n",
    "\n",
    "stringOut = str(out_4,'utf-8') # Bytes --> String\n",
    "cleanString = re.sub(r'[^A-ZÅÄÖa-zåäö \\.?!]', '', stringOut) # Remove all non characters (-space)\n",
    "nlString = re.sub(r'[ ]', r'\\n', cleanString) # New line instead of space\n",
    "\n",
    "text_file = open(\"normSelmaCorpus.txt\", \"w\")\n",
    "text_file.write(nlString)\n",
    "text_file.close()\n",
    "\n",
    "normselmaString = open('normSelmaCorpus.txt', 'r')\n",
    "\n",
    "normData = normselmaString.readlines()\n",
    "normData = [ x.rstrip() for x in normData ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nils',\n",
       " 'Holgerssons',\n",
       " 'underbara',\n",
       " 'resa',\n",
       " 'genom',\n",
       " 'Sverige',\n",
       " 'Selma',\n",
       " 'Lagerlöf',\n",
       " 'Innehåll',\n",
       " 'Den',\n",
       " 'kristna',\n",
       " 'dagvisan',\n",
       " '',\n",
       " 'Sveriges',\n",
       " 'karta',\n",
       " 'I',\n",
       " '.',\n",
       " 'Pojken',\n",
       " '',\n",
       " 'Tomten']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normData[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Modify your program to remove the punctuation signs and set all the text in lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "delimitNormData = normData.copy()\n",
    "added = 0\n",
    "\n",
    "for i, word in enumerate(normData):\n",
    "    upperCase = re.findall(r'^[A-Z]', word) # Remove all non characters (-space)\n",
    "\n",
    "    if upperCase:\n",
    "        delimitNormData[i + added] = re.sub(r'(\\p{Lu})', toLowercase, word)\n",
    "        delimitNormData.insert(i + added, \"<s>\")\n",
    "        added = added + 1\n",
    "  \n",
    "    dot = re.findall(r'\\.', word) # Remove all non characters (-space)\n",
    "    if dot:\n",
    "        delimitNormData.insert(i + added + 1, \"</s>\")\n",
    "        del delimitNormData[i + added]\n",
    "    \"\"\"\n",
    "    For removing characters other then letters\n",
    "    noLetters = re.findall(r'\\p{L}', delimitNormData[i + added])\n",
    "    if not noLetters:\n",
    "        del delimitNormData[i + added]\n",
    "        added = added - 1\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The result should be a normalized text without punctuation signs where all the sentences are delimited with < s> and < /s> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "flag = 0 # 0 = sentence in ongoing, 1 = finished sentence\n",
    "sentence = \"\"\n",
    "\n",
    "for word in delimitNormData:\n",
    "    if word == \"<s>\":\n",
    "        continue\n",
    "    if word == \"</s>\":\n",
    "        flag = 1\n",
    "        sentences.append(\"<s>\" + sentence + \"</s>\")\n",
    "        sentence = \"\"\n",
    "        continue\n",
    "    sentence = sentence + \" \" + word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. The five last lines of the text should look like this:\n",
    "\n",
    "< s > hon hade fått större kärlek av sina föräldrar än någon annan han visste och sådan kärlek måste vändas i välsignelse < /s >\n",
    "\n",
    "< s > när prästen sa detta kom alla människor att se bort mot klara gulla och de förundrade sig över vad de såg < /s >\n",
    "\n",
    "< s > prästens ord tycktes redan ha gått i uppfyllelse < /s >\n",
    "\n",
    "< s > där stod klara fina gulleborg ifrån skrolycka hon som var uppkallad efter själva solen vid sina föräldrars grav och lyste som en förklarad < /s >\n",
    "\n",
    "< s > hon var likaså vacker som den söndagen då hon gick till kyrkan i den röda klänningen om inte vackrare < /s >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> hon hade fått större kärlek av sina föräldrar än någon annan han visste  och sådan kärlek måste vändas i välsignelse</s>',\n",
       " '<s> när prästen sa detta  kom alla människor att se bort mot klara gulla  och de förundrade sig över vad de såg</s>',\n",
       " '<s> prästens ord tycktes redan ha gått i uppfyllelse</s>',\n",
       " '<s> där stod klara fina gulleborg ifrån skrolycka  hon  som var uppkallad efter själva solen  vid sina föräldrars grav och lyste som en förklarad</s>',\n",
       " '<s> hon var likaså vacker som den söndagen  då hon gick till kyrkan i den röda klänningen  om inte vackrare</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[len(sentences)-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "printOut = \"\"\n",
    "\n",
    "for sentence in sentences:\n",
    "    printOut = printOut + sentence\n",
    "    \n",
    "text_file = open(\"selmaLow.txt\", \"w\")\n",
    "text_file.write(printOut)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Estimate roughly the accuracy of your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = re.findall(r'[\\.\\!\\?]', strText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56097 found sentences and estimated sentences is 63698. \n",
      "This gives an estiamted accuaracy of 0.88067129266225\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \" + str(len(sentences)) + \" found sentences and estimated sentences is \" + str(len(dot)) + \". \\nThis gives an estiamted accuaracy of \" + str(len(sentences)/len(dot))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read and try programs to compute the frequency of unigrams and bigrams of the training set: [Program folder] (https://github.com/pnugues/ilppp/tree/master/programs/ch05/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python count.py < Selma.txt\n",
      "There are 39356 unigrams in training set.\n",
      "Most common words:\n",
      "och \t 37799\n",
      "att \t 28914\n",
      "han \t 22743\n",
      "det \t 22087\n",
      "i \t 17072\n",
      "som \t 16790\n",
      "hade \t 14955\n",
      "på \t 14634\n",
      "hon \t 14093\n"
     ]
    }
   ],
   "source": [
    "cmd_5 = \"python count.py < Selma.txt\"\n",
    "print(cmd_5)\n",
    "\n",
    "p = subprocess.Popen(cmd_5, stdout=subprocess.PIPE, shell=True)\n",
    "out_5, err = p.communicate() \n",
    "\n",
    "stringOut = str(out_5,'utf-8') # Bytes --> String\n",
    "lines = len(re.findall(r'\\n', stringOut))\n",
    "\n",
    "print(\"There are \" + str(lines) + \" unigrams in training set.\")\n",
    "print(\"Most common words:\\n\" + stringOut[:105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python count_bigrams.py < Selma.txt\n",
      "There are 319877 bigrams in training set.\n",
      "Some bigrams:\n",
      "3 \t ('nils', 'holgerssons')\n",
      "1 \t ('holgerssons', 'underbara')\n",
      "1 \t ('underbara', 'resa')\n",
      "4 \t ('resa', 'genom')\n",
      "3 \t ('genom', 'sverige')\n",
      "1 \t ('sverige', 'selma')\n",
      "11 \t ('selma', 'lagerlöf')\n",
      "2 \t ('lagerlöf', 'innehåll')\n"
     ]
    }
   ],
   "source": [
    "cmd_6 = \"python count_bigrams.py < Selma.txt\"\n",
    "print(cmd_6)\n",
    "\n",
    "p = subprocess.Popen(cmd_6, stdout=subprocess.PIPE, shell=True)\n",
    "out_6, err = p.communicate() \n",
    "\n",
    "stringOut = str(out_6,'utf-8') # Bytes --> String\n",
    "lines = len(re.findall(r'\\n', stringOut))\n",
    "\n",
    "print(\"There are \" + str(lines) + \" bigrams in training set.\")\n",
    "print(\"Some bigrams:\\n\" + stringOut[:214])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is the possible number of bigrams and their real number? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are 965 943 words in the corpus the maximum possible number of bigrams is 965 943 * 2 - 2 = 1 931 884"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Their real value for selma corpus is, as printed above, 319 877. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain why such a difference. What would be the possible number of 4-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a difference of (1 931 884 - 319 877 = ) 1 612 007 \"missing\" bigrams. This is because of there is many bigrams that are occuring more than once. For example \"selma lagerlöf\" is occuring 11 times, as seen printed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Propose a solution to cope with bigrams unseen in the corpus. This topic will be discussed during the lab session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the likelihood of a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Write a program to compute a sentence's probability using unigrams. You may find useful the dictionaries that we saw in the mutual information program: [Program folder]. (https://github.com/pnugues/ilppp/tree/master/programs/ch05/python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python mutual_info.py < Selma.txt\n"
     ]
    }
   ],
   "source": [
    "cmd_7 = \"python mutual_info.py < Selma.txt\"\n",
    "print(cmd_7)\n",
    "\n",
    "p = subprocess.Popen(cmd_7, stdout=subprocess.PIPE, shell=True)\n",
    "out_7, err = p.communicate() \n",
    "\n",
    "stringOut = str(out_7,'utf-8') # Bytes --> String\n",
    "\n",
    "#print(stringOut[17851173-2000:17851173])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17851173"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stringOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nils Holgerssons underbara resa genom Sverige\n",
      "Selma Lagerlöf\n",
      "\n",
      "Innehåll\n",
      "\tDen kristna dagvisan - Sveriges karta\n",
      "I.\tPojken - Tomten - Vildgässen - Det rutiga tygstycket\n",
      "II.\tAkka från Kebnekajse - Kvällen - Natten - Gåsleken\n",
      "III.\tVildfågelsliv - I bondgården - Vittskövle - I Övedsklostrets park\n",
      "IV.\tGlimmingehus - Svartråttor och gråråttor - Storken - Råttfångaren\n",
      "V.\tDen stora trandansen på Kullaberg\n",
      "VI.\tI regnväder\n",
      "VII.\tTrappan med de tre trappstegen\n",
      "VIII.\tVid Ronneby å\n",
      "IX.\tKarlskrona\n",
      "X.\tResa till Ö\n"
     ]
    }
   ],
   "source": [
    "print(strText[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string2dict(text):\n",
    "    \"\"\"\n",
    "    Creates a dict with (word:number of apperences) from input string\n",
    "    \n",
    "    Input string, string\n",
    "    Output dict\n",
    "    \"\"\"\n",
    "    \n",
    "    stringList = re.findall(r\"\\p{L}+\",text)\n",
    "    stringDict = {key : 0 for key in stringList}\n",
    "    \n",
    "    stringDict = {</s> }\n",
    "    \n",
    "    for m in re.finditer(r\"\\p{L}+\", text): # Iterate thorugh every word\n",
    "        s = m.start()\n",
    "        e = m.end()\n",
    "        word = text[s:e]\n",
    "        stringDict[word] = stringDict[word] + 1\n",
    "        \n",
    "    for m in re.finditer(r\"</s>\", text): # Iterate thorugh every word\n",
    "        s = m.start()\n",
    "        e = m.end()\n",
    "        word = text[s:e]\n",
    "        stringDict[word] = stringDict[word] + 1\n",
    "\n",
    "    return stringDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'</s>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-f3db64bcdff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'selmaLow.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictWord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring2dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-c17eca58c804>\u001b[0m in \u001b[0;36mstring2dict\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mstringDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstringDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '</s>'"
     ]
    }
   ],
   "source": [
    "text = open('selmaLow.txt').read()  \n",
    "\n",
    "dictWord = string2dict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictWord[\"nils\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probModelUnigram(words, sentence):\n",
    "    \"\"\"\n",
    "    Returns prob of sentence\n",
    "    \n",
    "    :param dict:\n",
    "    :param string:\n",
    "    :return: matrix (wi C(wi) #words P(wi))\n",
    "    \"\"\"\n",
    "    \n",
    "    listSentence = textLowerList(sentence)\n",
    "        \n",
    "    uniModel = {key : list([]) for key in listSentence}\n",
    "    nbrWords = 965943\n",
    "    \n",
    "    for word in listSentence:\n",
    "        uniModel[word] = [word , words[word], nbrWords, words[word]/nbrWords]\n",
    "    \n",
    "    \n",
    "    return uniModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'det'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-2891c1676959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobModelUnigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictWord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"det var en gång en katt som hette nils </s>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-64-9e01c265c3ef>\u001b[0m in \u001b[0;36mprobModelUnigram\u001b[0;34m(words, sentence)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistSentence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0muniModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbrWords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnbrWords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'det'"
     ]
    }
   ],
   "source": [
    "uni = probModelUnigram(dictWord, \"det var en gång en katt som hette nils </s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wi, C(wi), #words, P(wi)] : ['det', 22087, 965943, 0.02286573845454649]\n"
     ]
    }
   ],
   "source": [
    "print(\"[wi, C(wi), #words, P(wi)] : \" + str(uni['det']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totModelUnigram(probDict):\n",
    "    sentenceProb = 1\n",
    "    \n",
    "    for words in probDict:\n",
    "        sentenceProb = sentenceProb * probDict[words][3]\n",
    "    \n",
    "    return sentenceProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.766766766226595e-28"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totModelUnigram(uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a program to compute the sentence probability using bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select five sentences in your test set and run your programs on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tabulate your results as in the examples below with the sentence \"Det var en gång en katt som hette Nils\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model\n",
      "=====================================================\n",
      "wi C(wi) #words P(wi)\n",
      "=====================================================\n",
      "det 22086 1086836 0.02032137323386417\n",
      "var 12852 1086836 0.011825151172762036\n",
      "en 13921 1086836 0.012808740233117047\n",
      "gång 1332 1086836 0.0012255758918548888\n",
      "en 13921 1086836 0.012808740233117047\n",
      "katt 15 1086836 1.3801530313681181e-05\n",
      "som 16790 1086836 0.015448512931113802\n",
      "hette 107 1086836 9.845091623759242e-05\n",
      "nils 84 1086836 7.728856975661462e-05\n",
      "</s> 62283 1086836 0.057306714168467\n",
      "=====================================================\n",
      "Prob. unigrams:   4.4922846219128876e-27\n",
      "Geometric mean prob.: 0.0023187115559242404\n",
      "Entropy rate:   8.752460922513437\n",
      "Perplexity:   431.2739967353978\n",
      "\n",
      "\n",
      "Bigram model\n",
      "=====================================================\n",
      "wi wi+1 Ci,i+1 C(i) P(wi+1|wi)\n",
      "=====================================================\n",
      "<s> det 5913 62283 0.09493762342854391\n",
      "det var 4023 22086 0.1821515892420538\n",
      "var en 753 12852 0.05859010270774977\n",
      "en gång 695 13921 0.04992457438402414\n",
      "gång en 23 1332 0.017267267267267267\n",
      "en katt 5 13921 0.0003591695998850657\n",
      "katt som 2 15 0.13333333333333333\n",
      "som hette 50 16790 0.0029779630732578916\n",
      "hette nils 0 107 0.0 *backoff: 7.728856975661462e-05\n",
      "nils </s> 2 84 0.023809523809523808\n",
      "=====================================================\n",
      "Prob. bigrams: 2.292224542392586e-19\n",
      "Geometric mean prob.: 0.013678098151101147\n",
      "Entropy rate: 6.191988542790593\n",
      "Perplexity: 73.10957919390972\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Unigram model\n",
    "=====================================================\n",
    "wi C(wi) #words P(wi)\n",
    "=====================================================\n",
    "det 22086 1086836 0.02032137323386417\n",
    "var 12852 1086836 0.011825151172762036\n",
    "en 13921 1086836 0.012808740233117047\n",
    "gång 1332 1086836 0.0012255758918548888\n",
    "en 13921 1086836 0.012808740233117047\n",
    "katt 15 1086836 1.3801530313681181e-05\n",
    "som 16790 1086836 0.015448512931113802\n",
    "hette 107 1086836 9.845091623759242e-05\n",
    "nils 84 1086836 7.728856975661462e-05\n",
    "</s> 62283 1086836 0.057306714168467\n",
    "=====================================================\n",
    "Prob. unigrams:   4.4922846219128876e-27\n",
    "Geometric mean prob.: 0.0023187115559242404\n",
    "Entropy rate:   8.752460922513437\n",
    "Perplexity:   431.2739967353978\n",
    "\n",
    "\n",
    "Bigram model\n",
    "=====================================================\n",
    "wi wi+1 Ci,i+1 C(i) P(wi+1|wi)\n",
    "=====================================================\n",
    "<s> det 5913 62283 0.09493762342854391\n",
    "det var 4023 22086 0.1821515892420538\n",
    "var en 753 12852 0.05859010270774977\n",
    "en gång 695 13921 0.04992457438402414\n",
    "gång en 23 1332 0.017267267267267267\n",
    "en katt 5 13921 0.0003591695998850657\n",
    "katt som 2 15 0.13333333333333333\n",
    "som hette 50 16790 0.0029779630732578916\n",
    "hette nils 0 107 0.0 *backoff: 7.728856975661462e-05\n",
    "nils </s> 2 84 0.023809523809523808\n",
    "=====================================================\n",
    "Prob. bigrams: 2.292224542392586e-19\n",
    "Geometric mean prob.: 0.013678098151101147\n",
    "Entropy rate: 6.191988542790593\n",
    "Perplexity: 73.10957919390972\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an application of n-grams, execute the Jupyter notebook by Peter Norvig here (http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb). Just run all the cells and be sure that you understand the code. You will find the data here (http://norvig.com/ngrams/). In you report, you will describe one experiment with a long string of words your will create yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a complement, you can read a paper by Church (https://researcher.watson.ibm.com/researcher/view.php?person=us-kwchurch) and Hanks, Word Association Norms, Mutual Information, and Lexicography, (https://www.aclweb.org/anthology/J90-1003) Computational Linguistics, 16(1):22-29, 1990, as well as another one on backoff by Brants et al. (2007) Large language models in machine translation (https://www.aclweb.org/anthology/D07-1090).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
