{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laboration 2 in EDAN20 @ LTH - http://cs.lth.se/edan20/coursework/assignment-2/\n",
    "\n",
    "Author: Jonatan Kronander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "* Write a program to find n-gram statistics\n",
    "* Compute the probability of a sentence\n",
    "* Know what a language model is\n",
    "* Write a short report of 1 to 2 pages on the assignment\n",
    "* Optionally read a short article on the importance of corpora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Collect a corpus of at least 750,000 words. Alternatively, you can retrieve a corpus of novels by Selma Lagerlöf from this URL: http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/Selma.txt ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/Selma.txt\").read() # Open file and read\n",
    "strText = str(text,'utf-8') # From bytes to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = open(\"Selma.txt\", \"w\")\n",
    "text_file.write(strText)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will check the number of words using the Unix command wc -w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in text:   965943 Selma.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output = os.popen('wc -w Selma.txt').read()\n",
    "print(\"Number of words in text:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the concordance program (https://github.com/pnugues/ilppp/tree/master/programs/ch02/python) to print the lines containing a specific word, for instance Nils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python concord.py Selma.txt \\bNils\\b 10\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "pattern = r\"\\bNils\\b\"\n",
    "width = \" 10\"\n",
    "cmd = \"python concord.py Selma.txt \"+pattern+width\n",
    "print(cmd)\n",
    "\n",
    "p = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
    "out, err = p.communicate() \n",
    "\n",
    "print(out)\n",
    "\n",
    "#result = out.split('\\n')\n",
    "#for lin in result:\n",
    "#    if not lin.startswith('#'):\n",
    "#        print(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the tokenization program (https://github.com/pnugues/ilppp/tree/master/programs/ch05/python) on your corpus and count the words using the Unix sort and uniq commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def toLowercase(matchobj):\n",
    "    \"\"\"\n",
    "    Helper function\n",
    "    \"\"\"\n",
    "    return matchobj.group(1).lower()\n",
    "\n",
    "def textLowerList(text):\n",
    "    \"\"\"\n",
    "    Make text lowercase and put into list\n",
    "    \n",
    "    :param string:\n",
    "    :return list:\n",
    "    \"\"\"\n",
    "    \n",
    "    textLow = re.sub(r'(\\p{Lu})', toLowercase, text) # Lowercase all characters\n",
    "    \n",
    "    stringList = re.findall(r\"\\p{L}+\",textLow) # This finds all words from a txt file. r\"[a-zåäö]+ equal to r\"\\w+\" \n",
    "    \n",
    "    return stringList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "text = urlopen(\"http://fileadmin.cs.lth.se/cs/Education/EDAN20/corpus/Selma.txt\").read() # Open file and read\n",
    "strText = str(text,'utf-8') # From bytes to string\n",
    "\n",
    "stringList = textLowerList(strText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing a corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a program to insert <s> and </s> tags to delimit sentences. You can start from the tokenization and modify it. Use a simple heuristics such as: a sentence starts with a capital letter and ends with a period. Estimate roughly the accuracy of your program.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Modify your program to remove the punctuation signs and set all the text in lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The result should be a normalized text without punctuation signs where all the sentences are delimited with <s> and </s> tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The five last lines of the text should look like this:\n",
    "\n",
    "< s > hon hade fått större kärlek av sina föräldrar än någon annan han visste och sådan kärlek måste vändas i välsignelse < /s >\n",
    "\n",
    "< s > när prästen sa detta kom alla människor att se bort mot klara gulla och de förundrade sig över vad de såg < /s >\n",
    "\n",
    "< s > prästens ord tycktes redan ha gått i uppfyllelse < /s >\n",
    "\n",
    "< s > där stod klara fina gulleborg ifrån skrolycka hon som var uppkallad efter själva solen vid sina föräldrars grav och lyste som en förklarad < /s >\n",
    "\n",
    "< s > hon var likaså vacker som den söndagen då hon gick till kyrkan i den röda klänningen om inte vackrare < /s >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read and try programs to compute the frequency of unigrams and bigrams of the training set: [Program folder] (https://github.com/pnugues/ilppp/tree/master/programs/ch05/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is the possible number of bigrams and their real number? Explain why such a difference. What would be the possible number of 4-grams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Propose a solution to cope with bigrams unseen in the corpus. This topic will be discussed during the lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the likelihood of a sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a program to compute a sentence's probability using unigrams. You may find useful the dictionaries that we saw in the mutual information program: [Program folder]. (https://github.com/pnugues/ilppp/tree/master/programs/ch05/python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a program to compute the sentence probability using bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Select five sentences in your test set and run your programs on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tabulate your results as in the examples below with the sentence \"Det var en gång en katt som hette Nils\":\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram model\n",
    "=====================================================\n",
    "wi C(wi) #words P(wi)\n",
    "=====================================================\n",
    "det 22086 1086836 0.02032137323386417\n",
    "var 12852 1086836 0.011825151172762036\n",
    "en 13921 1086836 0.012808740233117047\n",
    "gång 1332 1086836 0.0012255758918548888\n",
    "en 13921 1086836 0.012808740233117047\n",
    "katt 15 1086836 1.3801530313681181e-05\n",
    "som 16790 1086836 0.015448512931113802\n",
    "hette 107 1086836 9.845091623759242e-05\n",
    "nils 84 1086836 7.728856975661462e-05\n",
    "</s> 62283 1086836 0.057306714168467\n",
    "=====================================================\n",
    "Prob. unigrams:   4.4922846219128876e-27\n",
    "Geometric mean prob.: 0.0023187115559242404\n",
    "Entropy rate:   8.752460922513437\n",
    "Perplexity:   431.2739967353978\n",
    "\n",
    "\n",
    "Bigram model\n",
    "=====================================================\n",
    "wi wi+1 Ci,i+1 C(i) P(wi+1|wi)\n",
    "=====================================================\n",
    "<s> det 5913 62283 0.09493762342854391\n",
    "det var 4023 22086 0.1821515892420538\n",
    "var en 753 12852 0.05859010270774977\n",
    "en gång 695 13921 0.04992457438402414\n",
    "gång en 23 1332 0.017267267267267267\n",
    "en katt 5 13921 0.0003591695998850657\n",
    "katt som 2 15 0.13333333333333333\n",
    "som hette 50 16790 0.0029779630732578916\n",
    "hette nils 0 107 0.0 *backoff: 7.728856975661462e-05\n",
    "nils </s> 2 84 0.023809523809523808\n",
    "=====================================================\n",
    "Prob. bigrams: 2.292224542392586e-19\n",
    "Geometric mean prob.: 0.013678098151101147\n",
    "Entropy rate: 6.191988542790593\n",
    "Perplexity: 73.10957919390972\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an application of n-grams, execute the Jupyter notebook by Peter Norvig here (http://nbviewer.jupyter.org/url/norvig.com/ipython/How%20to%20Do%20Things%20with%20Words.ipynb). Just run all the cells and be sure that you understand the code. You will find the data here (http://norvig.com/ngrams/). In you report, you will describe one experiment with a long string of words your will create yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a complement, you can read a paper by Church (https://researcher.watson.ibm.com/researcher/view.php?person=us-kwchurch) and Hanks, Word Association Norms, Mutual Information, and Lexicography, (https://www.aclweb.org/anthology/J90-1003) Computational Linguistics, 16(1):22-29, 1990, as well as another one on backoff by Brants et al. (2007) Large language models in machine translation (https://www.aclweb.org/anthology/D07-1090).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
